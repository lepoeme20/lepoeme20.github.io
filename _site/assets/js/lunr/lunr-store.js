var store = [{
        "title": "CURRICULUM VITAE",
        "excerpt":"  Contact Information  Ph.D Candidate School of Industrial Management Engineering, Korea University New engineering hall 220, 145 Anam-Ro, Seongbuk-gu, Seoul 02841, Republic of Korea   Mobile: +82) 10-8589-286 E-mail: lepoeme20@gmail.com Github: https://github.com/lepoeme20 Tech blog: https://lepoeme20.github.io LinkedIn: https://www.linkedin.com/in/lepoeme       Education       Seoul  Korea University  2017 - current  Ph.D candidate in Industrial Management    Seoul  Seoul Tech  2011 - 2017  Bachelor of Science in Information Technology and Management        Research Interests   Anomaly Detection   - Use anomaly detection to solve real world problems in domains with few target labels     Adversarial Example   - Making human-imperceptable adversarial examples which is an instance with small, intentional feature perturbations that cause a machine learning model to make a false prediction   - Detection/defence a well made adversarial examples    Vision Process   - Image(video) classification, object detection and generation        Publications       2020   Unusual customer response identification and visualization based on text mining and anomaly detection   Seungwan Seo, Deokseong Seo, Myeongjun Jang, Jaeyun Jeong, Pilsung Kang*  Expert Systems with Applications, Volume 144, April 2020, 113111, IF: 5.452, Q1 Journal  Original paper link | Download link    2020   Comparative Study of Deep Learning-Based Sentiment Classification   Seungwan Seo, Czangyeob Kim, Haedong Kim, Kyounghyun Mo, Pilsung Kang*  IEEE Access, Volume 8, January 2020, Pages 6861 - 6875, IF: 3.745, Q1 Journal  Original paper link | Download link    2019   Recurrent neural network-based semantic variational autoencoder for Sequence-to-sequence learning   Myeongjun Jang, Seungwan Seo, Pilsung Kang*  Information Sciences, Volume 490, July 2019, Pages 59-73, IF: 5.910, Q1 Journal  Original paper link | Download link    2019   Sentiment classification with word localization based on weakly supervised learning with a convolutional neural network   Gichang Lee, Jaeyun Jeong, Seungwan SeoSeo, CzangYeob Kim, Pilsung Kang*  Knowledge-Based Systems Volume 152, 15 July 2018, Pages 70-82, IF: 5.921, Q1 Journal  Original paper link | Download link       Work in Progress       Under Review   Intrusion Detection based on Sequential Information preserving Log Embedding Methods and Anomaly Detection Algorithms   Czangyeob Kim, Myeongjun Jang, Seungwan Seo, Kyeongchan Park, Pilsung Kang*  EEE Access, IF: 3.7451        Conference       2019. 11   Intrusion detection based on sequential information preserving log embedding methods and anomaly detection algorithms   Seungwan Seo, Pilsung Kang*  INFORMS Annual Meeting, Seattle, WA 2019.11     2019. 11   Measuring Sentence Similarity based on Image Association and Siamese Network   Seungwan Seo,  Minsung Jeong, Heejeong Choi, Pilsung Kang*  Korean Institute of Industrial Engineers (Fall), Seoul National University, Seoul     2018. 11   Identifying and visualizing uncommon customer response on machine learning   Seungwan Seo, Pilsung Kang*  INFORMS Annual Meeting, Phoenix, Arizona 2018.11     2018. 04   Comparative Study of Deep Learning-Based Sentiment Classification   Seungwan Seo, Czangyeob Kim, Haedong Kim, Kyounghyun Mo, Pilsung Kang*  Korean Institute of Industrial Engineers (Spring), Hyundai Hotel, Gyeongju     2018. 04   Unusual customer response identification and visualization based on text mining and anomaly detection   Seungwan Seo, Deokseong Seo, Myeongjun Jang, Jaeyun Jeong, Pilsung Kang*  Korean Institute of Industrial Engineers (Spring), Hyundai Hotel, Gyeongju     2018. 04   Distance Decomposition for Variable Importance of Distance-based Novelty Detectionn   Seungwan Seo, Myeongjun Jang, Jaeyun Jeong, Pilsung Kang*  Korean Institute of Industrial Engineers (Spring), Hyundai Hotel, Gyeongju     2018. 04   Recurrent Neural Network-based Semantic Variational Autoencoder for Sequence to Sequence Learning   Seungwan Seo, Deokseong Seo, Myeongjun Jang, Jaeyun Jeong, Pilsung Kang*  Korean Institute of Industrial Engineers (Spring), Hyundai Hotel, Gyeongju     2017. 11   Sentiment classification with word localization based on weakly supervised learning with a convolutional neural network   Seungwan Seo, Gichang Lee, Jaeyun Jeong, Czangyeob Kim, Pilsung Kang*  Korean Institute of Industrial Engineers (Fall), KAIST, Daejeon 2017.11        Projects   Research Projects     2020. 10 - current   Research on methodology to improve ocean unseen data classification accuracy   Daewoo Shipbuilding &amp; Marine Engineering Co., Ltd.   Daewoo Shipbuilding &amp; Marine Engineering Co., Ltd.     2019. 03 - current   Research on methodology for DeepFake video detection   National Research Foundation of Korea   - Development of DeepFake video detection model through classification and anomaly detection     2020. 07 - 2020. 12   Fashion image detection and Recommendation   National Information Society Agency   - Develop fashion style classification model using image detection model     2019.09 - 2020.05   Research on methodology to improve ocean unseen data classification accuracy   Daewoo Shipbuilding &amp; Marine Engineering Co., Ltd.   - Ocean image generation using GANs for improve ocean unseen data classificaon accuracy     2019.04 - 2019.09   Research on noise-robustness pattern detection from game log   NCSoft   - Consider only sequences that appear continuously, which was not possible with existing pattern detection methods     2018.06 - 2019.02   Mesuring sentence similarity based on image association   NCSoft   - Judging whether two sentences are similar through image association, just as a person judges whether two sentences are similar through associative action    2018.03 - 2019.03   Detecting abnormal state of system based on machine learning   Agency for Defense Development   - Detects abnormal conditions using RNN-based auto encoder     2018.03 - 2018.09   Research on network abnormal communication identification technology based on unsupervised learning   National Security Research Institute   - Detection of attack network communication using anomaly detection     2017.11 - 2018.04   Sentiment analysis for news articles   Signal Korea   - Development of deep learning-based model that classifies the polarity of news articles and comments    2017.07 - 2018.01   Development of deep learning and text mining based VDS analysis system   Hyundai Motors   - Using Doc2Vec and novelty detection technique for VDS response, quantitative score calculation for each response     2017.04 - 2018.03   Sentiment analysis and visualization of customers review data   National Research Foundation of Korea   - Development of sentiment analysis model for Korean review data and construction of visualization site         Company Projects     2020.11.10 - 2019.12.15   i-TAP BMW bad wafer clustering   SK hynix   - Wafer map clustering to find bad patterns     2020.02.03 - 2020.03.27   Big Data Expert Training Course   SK hynix   Team 1: FDC-based Production Process Response Prediction   Team 2: Deep Learning-based Wafer Bit Map Classifier     2019.11.06 - 2019.12.10   Data Analysis Expert Project LG Innotek   SK hynix   Team 1: Development of machine learning based virtual metrology for sterilization simulator   Team 2: Improving Epoxy coating quality by ensuring Epoxy quantitative discharge conditions        Teaching Experiences       2020.11.5, 10, 17, 24   Machine Learning and Deep Learning Course   SK Hynix   Full-time Lecture     2020.03.17, 18   Machine Learning and Deep Learning Course   SK Hynix   Full-time Lecture     2019.09.09   Machine Learning with Python   Korea Credit Bureau   Full-time Lecture     2019.08.26   Machine Learning with Python   Korea Credit Bureau   Full-time Lecture    2019.07.15, 19   Machine Learning and Deep Learning Course   Samsung Electronics   Full-time Lecture     2019.02.22   Deep Learning for Text Domain   SeoulTech   Seminar     2019.02.11   Machine Learning and Deep Learning Course   Samsung Electronics    Full-time Lecture     2019.01.14   Python Syntax Course   Samsung Electronics    Full-time Lecture        Reference   Pilsung Kang   Associate Professor, Department of Industrial Engineering, Korea University   Ph.D, Industrial Engineering, Seoul National University    Mobile: +82) 2-3290-3383 E-mail: pilsung_kang@korea.ac.kr   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/portfolio/seungwan_cv/",
        "teaser":"http://localhost:4000/assets/images/docker/docker-logo.png"},{
        "title": "Deep learning을 속이는 기술: Adverarial example (paper: Explaining and Harnessing Adversarial Examples)",
        "excerpt":"AGENDA 1. What are the adversarial examples 2. Explaining and Harnessing Adversarial Examples 논문 설명   Adversarial example에 대하여 생소하신 분들이 있으실 수 있으니, 우선 간략히 해당 개념에 대하여 영상과 함께 설명 드리겠습니다. 그 후 본 포스트의 주제인 Explaining and Harnessing Adversarial Examples 논문에 대하여 설명 드리겠습니다.   Adversarial example이란 무엇인가?     위의 영상을 보시면 바나나가 있습니다. 바나나가 홀로 있을 때와, 영상 초반에 깨끗해 보이는 이미지 패치 (사람이 들고 있는 작은 이미지)를 바나나 옆에 두었을 때는 학습해둔 모델이 100%에 가까운 confidence로 바나나를 분류 하고 있습니다(초록색 그래프). 하지만 영상 중반부에 나오는 이상하게 생긴 이미지 패치를 바나나 옆에 두게 되면 모델은 이전과 다르게 100%에 가까운 confidence로 바나나를 토스터기로 오분류 하고 있음을 볼 수 있죠. 이와 같이 모델이 오분류를 하게끔 하는 패치를 “Adversarial patch”, 모델이 오분류를 하게끔 noise를 더한 이미지를 adversarial example이라 부릅니다.   Adversarial example의 통상적인 정의는 아래와 같습니다:     Original input에 매우 작은 noise를 더하여(perturbation) 사람의 눈에는 차이가 없어 보이나 분류기는 잘못 분류 하도록 변형된 input    Adversarial example을 만들어 내는 행위를 adversarial attack이라 칭하며, 이를 방어하는 행위는 defense against adversarial attack 이라 부릅니다.  Adversarial attack의 종류와 의미는 다음과 같습니다(여기서 말하는 종류는 세세한 모델이 아니라 큰 흐름을 의미합니다):      White-box attack - 공격자가 target model (e.g. classifier)의 parameters를 알고 있는 상황   Black-box attack - 공격자가 target model (e.g. classifier)의 parameters를 모르는 상황   Targeted attack - Adversarial example이 공격자가 원하는 class로 target classifier가 오분류 하도록 공격하는 방식   Untargeted attack - Adversarial example이 true class가 아닌 어떠한 class로 target classifier가 오분류 하도록 공격하는 방식   한 가지 더 언급할 점은 black-box attack의 경우 공격자가 target model의 parameter를 모르기 때문에 공격이 불가능 합니다. 이에 공격자가 가지고 있는 데이터를 이용하여 target model의 분류 경계면을 근사하는 모델을 하나 만들어 낸 후 해당 모델의 parameter를 이용하여 adversarial example을 생성하는 flow를 가집니다. 이러한 특성상 black-box attack이 방어를 하는 입장에서는 white-box attack보다 쉬운 경향을 띄게 됩니다.         Fig 1. Examples of adversarial examples  Fig 1.에서 위쪽 행은 clean images이며 아래쪽 행은 adversarial examples입니다. 이미지 아래 보이는 숫자와 기호는 모델이 위의 이미지를 보고 분류한 class를 나타내고 있습니다. 자율주행 자동차가 카메라로 사물을 인식하며 움직이다가 위와 같이 adversarial example을 만나 오분류를 하게 된다면 정말 끔찍한 일이 아닐 수 없습니다. 이러한 문제 인식을 통하여 모델 자체(연구) 뿐만 아니라 실생활에서도 본 문제를 꼭 해결해야 한다는 경각심을 가질 수 있습니다.   Explaining and Harnessing Adversarial Examples   Adversarial example이 대략 이런거구나 라고 생각 하셨을테니, 이제 본격적으로 본 논문을 소개 드리도록 하겠습니다.   본 논문에서는 adversarial example을 연구함으로써 엔지니어들의 학습 algoritms에 대한 blind spots을 찾을 수 있다고 언급하고 있습니다. 저에게는 이 말이 보다 사람다운 모델과 사람다운 학습 방식을 찾아야 한다는 의미로 들리더군요. 또한 이전까지는 adversarial example에 대하여 모델은 완전하나 이를 사용하는 engineer가 매우 비선형적인 Deep Neural Network (DNN)을 제대로 이해하지 못하고 모델을 제대로 학습시키지 못했기 때문에 발생하는 특수 케이스라고 여겼습니다. 하지만 본 논문에서는 이러한 추측이 틀렸음을 실험을 통하여 보여주고 있죠.   한 가지 재미 있는 사실은 본 논문에서 activation fuction으로 기존에 사용하던 sigmoid 혹은 ReLU를 사용하지 않고 RBF를 사용하면 adversarial noise에 보다 강건한 모델이 됨을 언급하고 있습니다.     다시 말해 DNN은 우리가 생각하는 것 만큼 non-linear 하지 않습니다.    생각을 해본다면 ReLU의 경우 최적화를 위해 태생이 매우 linear한 구조를 띄고 있습니다. 또한 sigmoid의 경우에도 모델을 학습 하다 보면 선형성을 지닌 부분에 많은 activation이 된다는 언급도 본 논문의 저자들은 잊지 않고 있네요.   하지만 이렇게 말로써 우리가 비선형적이라고 생각했던 모델은 사실 선형적이었어! 라고 한다면 이렇게까지 주목받는 논문이 되지 못했겠죠. 저자들은 매우 선형적인 공격 방식으로 생성한 adversarial example이 ‘비선형적’ 이라고 알려진 DNN (maxout)을 얼마나 잘 속이는지 실험을 통해 보여줍니다. 본 논문에서 사용하는 공격 방식은 Fast Gradient Sign Method (FGSM)이라고 불리며 식은 아래와 같습니다:     FGSM:     \\(\\eta = \\epsilon(\\nabla_x J(\\theta, x, y))\\),     where J는 target model의 목적식, $\\theta$는 모델의 파라미터, x와 y는 각각 input과 output 이며 $\\epsilon$은 노이즈의 크기를 결정하는 상수    위에서 구한 $\\eta$를 원본 이미지에 더해주어 나온 이미지가 adversarial example이 되며 식은 아래와 같습니다:   \\[\\widetilde{x} = x + \\eta\\]   본 포스트의 상단부에서 말씀드린 adversarial example의 통상적인 정의에 따라 공격자는 사람이 인지하지 못할 만큼 작은 $\\eta$를 원본 이미지에 더하여 target model을 속이는 것을 목표로 하고 있습니다.         Fig 2. FGSM   위의 2번 이미지를 보면 57.7%의 confidence를 가지고 팬더를 팬더로 인식하는 모델을 속이기 위하여 FGSM 방식으로 얻은 noise를 원본 이미지에 더해주고 있습니다. 이러한 과정을 거쳐 나온 이미지, adversarial example은 모델이 99.3%의 confidence로 긴팔 원숭이로 분류하고 있음을 확인 할 수 있습니다. 심지어 생성된 노이즈는 8.2%의 confidence로 선형동물로 인식되는 매우 의미 없는 문자 그대로의 nosie인데도 말이죠. 그리고 위에서 노이즈 앞에 보이시는 0.007이라는 수가 위 식에서 보셨던 noise의 크기를 결정하는 $\\epsilon$입니다.   본 논문에서 저자들은 adversarial training이라는 개념을 통하여 이러한 공격을 어느정도 막을 수 있다고 언급하고 있습니다. 하지만 최근 연구 동향은 이러한 adversarial training이 여러 공격 패턴에 robust하지 못하다는 점을 조금 critical하게 생각하고 크게 사용하지 않는 추세 입니다. Adversarial training은 간략히 말해서 생성된 adversarial example을 모델이 학습하는 동안 clean data와 함께 학습시키는 방식 입니다. 직관적으로 생각해봐도 공격 방법이 늘어날수록 모든 공격방법으로 얻은 adversarial example을 획득한 다음 모든 data를 학습에 이용해야 하니 상당히 비효율 적일 것 같네요.   마무리   이로써 첫 번째 포스팅이 마무리 되었습니다. 어떠신가요? 저는 이 분야를 공부하면서 매우 새롭고 신비로웠습니다. 지금도 그렇구요. 여러분들도 조금의 흥미 그리고 믿고 있던 DNN에 대해서 회의를 느끼는 시간이 되셨으면 좋겠습니다.  ","categories": ["Adversarial example"],
        "tags": ["adversarial examples","adversarial attack","ICLR","Explaining and Harnessing Adversarial Examples","FGSM","fast gradient sign method"],
        "url": "http://localhost:4000/archive/FGSM",
        "teaser":null},{
        "title": "Introduction to Deep Learning-MIT 6.S191",
        "excerpt":"AGENDA 1. Introduction of basic concepts for Deep learning   이미 너무 유명하고 우리에게 친숙한 Stanford 에서 제공하는 CS231n 수업 외에도 deep learning을 학습하기 위한 좋은 material이 있지 않을까? 라는 생각에서 시작하여 2019년 MIT에서 제공하는 deep learning 수업을 review 해볼까 합니다. 전반적으로 CS231n보다 범위가 넓고 깊이가 얕은 특성이 있는 듯 합니다.   사족이지만 해당 class의 final project의 결과에 따라 우수한 학생에게는 RTX 2080 Ti를 상으로 제공한다고 하네요…. MIT 만세   Deep learning은 무엇인가?     “An intelligence as the ability to process information to inform future decision.”    수업 진행자가 하는 말을 그대로 가지고 와보았습니다. 이는 결국 데이터의 패턴을 잘 파악하여 미래 데이터에 적용한 후 정보를 추출하는 일련의 과정이 됩니다. 그렇다면 어떠한 점들이 machine learning과의 차이점일까요?         Fig 1. The differences between AI, ML, and DL   위의 그림은 Artificial intelligence (AI), machine learning (ML), 그리고 deep learning (DL)의 차이를 나타내고 있습니다. AI가 가장 큰 범위의 개념으로 기계가 사람과 같이 행동할 수 있는 모든 기술을 의미하고 있으며 ML은 AI에 속하는 개념으로 어느정도 자동화가 되어 학습된 패턴을 통하여 미래 데이터에 대하여 의사결정을 내리는 행위를 의미합니다. 마지막으로 최근 비약적인 속도로 발전하고 있는 DL분야는 ML에 속하는 굉장히 좁은 영역으로 ML에 비해 사람의 손을 덜 타는, 예를 들어 image domain에서 이미지를 잘 표현하기 위하여 여러 방법으로 hand-crafted features를 추출하는 것이 기존 ML의 중요한 이슈였다면 deep learning에서는 모델이 알아서 feature를 뽑도록 합니다, 보다 지능적인 프로세스 라고 표현할 수 있을 것 같습니다.   그렇다면 우리는 왜 deep learning을 해야 할까요?   가장 단순하게 이야기 하면 기존에 사용하던 hand-crafed features에 비하여 우리의 손을 거치지 않고 기계가 직접 중요하다고 판단한 features를 사용하는 것이 많은 분야에서 성능이 ‘훨씬’ 더 좋기 때문일 것입니다.   Feed forward propagation        Fig 2. MLP   일반적인 neural network의 feed forward propagation은 위 그림에서 보이는 \\(x_1, x_2, x_3\\)와 같은 input들과 \\(w_11, w_12, w_32\\)와 같은 weight들의 dot product 로 나온 값을 비선형 함수에 input으로 사용하는 과정들을 반복한 후 최종 output이 산출 됩니다. 이 과정에서 추가되는 bias term은 activation function을 shift하는 효과를 준다고 하네요.   Neural network는 사실 Fig 2.에서 가운데 위치하는 hidden layer를 여려층 쌓음으로 매우 복잡한 결합함수 형태를 가집니다. 만약 선형함수를 계속 결합한다면 어떻게 될까요? 최종적인 함수의 형태도 선형 함수가 되겠죠? 그렇다면 힘들게 쌓아올린 hidden layer들이 큰 의미를 갖기 어려울 것입니다. 이러한 문제를 해결하기 위하여 neural network에서는 비선형함수인 activation function을 사용하여 모델에 비선형성을 추가해 줍니다. 이러한 특성 덕분에 neural net은 매우 비선형적인 현실 데이터에서도 높은 성능을 지닐 수 있게 됩니다.   가장 유명하고 오래 쓰인 activation function은 sigmoid 함수 입니다. sigmoid 함수는 인풋을 0-1로 바꾸어주기 때문에 모델을 통하여 확률을 구하고자 하는 경우에 상당히 유용한 함수가 될것이라고 강의자는 말하고 있습니다. 또 하나의 유용하고 유명한 함수는 Rectified Liner Unit (ReLU) 입니다. ReLU의 경우 0보다 작은 input은 무시하고 0보다 큰 값에 대하여서는 인풋이 아웃풋이 되는 \\(y=x\\) 형태를 띄고 있습니다. 이러한 특성 때문에 picewise linearity라고 불리죠. 이렇게 단순한 함수로 충분한지 의심이 드실텐데요, ReLU는 굉장히 선형적으로 생겼음에도 불구하고 모든 중요한 속성을 보존함과 동시에 연산이 쉽기 때문에 최근 대부분의 모델에서 사용하고 있습니다.   Building neural network with Perceptrons  그림 2에서 \\(h_1\\)은 inputs과 weights를 사용한 weighted sum 형태가 됩니다. 이렇게 weighted sum이 된 값에 activation function을 거쳐 neuron 하나의 output이 산출되죠. Hidden unit이 많아져도 neuron마다 찬찬히 살펴보면 단순히 하나의 neuron을 가진 perceptron network (그림 2) 와 다를바가 없습니다. 물론 weights들의 값이 다르기 때문에 \\(h1, h2\\)의 값은 각각 다르겠지요. 이렇게 빽빽하게 weight가 연결된 구조를 우리는 dense layer라고 부릅니다. Deep Neural Network또한 이러한 구조와 크게 다르지 않은데, 단순히 계속적으로 엄청 많이 계속 계속 이러한 dense layer을 stack하게 되면 그것이 DNN이 됩니다.   Applying Neural Networks  굉장히 뜨겁고 모든 것을 해줄것만 같은 deep learning model또한 물론 만능이 아닙니다. 사람이 봤을때도 합리적인 판단을 모델이 하기 위해서 우리는 모델을 ‘잘’ 학습 시켜야만 하죠. 우리는 실제 정답을 ground truth, model이 예측한 정답을 predicted output이라 부르며 이 둘의 차이를 점차 줄여나가는 과정을 학습이라 부릅니다. 이처럼 실제 정답과 모델이 추측한 정답을 줄여나가기 위하여 세우는 함수인 Loss function (NLL, cross-entrophy, MSE와 같은 다양한 방법들이 있습니다)을 정의하는 것은 machine learning의 art에 속합니다.   Training Neural Networks  위에서 우리는 모델을 잘 학습 시켜야 한다고 말하였습니다. 그렇다면 학습을 시킨다는 것은 무엇을 의미하는 것일까요? 모델을 학습 시키는 궁극적인 목표는 모델이 가지고 있는 파라미터 W (weights)를 최적화 하는 것입니다. 여기서 최적화의 의미는 predicted output과 ground truth의 차이를 최소화 하는 것이 됩니다. 하지만 Neural network는 위에서 말한 것 처럼 상당히 많은 layer (function)가 stack되어 있기 때문에 최적해를 closed form으로 구할 수 없는 문제점을 가지고 있습니다. 이러한 어려운 상황에서 해를 구하기 위해서 우리는 gradient descent와 backpropagation을 사용합니다.   Neural Networks in practice  Optimization  DNN은 매우 복잡한 결합함수 형태를 띄고 있기 때문에 구조적으로 매우 많은 local minimum이 존재하고 initial point에 따라 이런 local minimum에 빠질수도, 저런 local minimum에 빠질 수도 있습니다. 이 때문에 optimization (adam, adadelta, RMSprop, …) 연구도 매우 활발히 진행 되었으나 최근에는 주춤하는 모습인것 같습니다. 본인이 선호하는 optimizer를 사용하는 추세이며 ‘local optimum에 빠지더라도 충분히 좋다’ 라는게 학계의 정설 입니다.   Overfitting  Model은 새로운 데이터 (test set)에 대해서도 강건함을 목표로 학습됩니다. 다시 말해, 처음 보는 데이터에 대해서도 학습때의 성능을 보여주길 바라며 모델을 학습 시키죠. 하지만 간혹 학습 데이터를 완전히 외워버리는 모델들이 등장하곤 하는데 이러한 경우 새로운 데이터에 대한 성능은 현저히 떨어질 수 밖에 없습니다. 학습 데이터를 온전히 외워버리기 때문에 새로운 데이터에 대해서 대처하지 못하는 상황이기 때문이죠. 이러한 현상을 우리는 overfitting이라 부릅니다.   Regulaization  이러한 overfitting 현상을 완화하기 위하여 사용하는 방법이 regularization입니다.  가장 유명하고 널리 사용되는 방법은 dropout으로 특정 neuron을 deactivation 시키는 방식입니다. 이러한 방식을 통하여 모델이 특정 witght에 온전히 의존하지 못하게 하는 것이죠.   두 번째 방식은 early stopping이라 불리는 트릭 입니다. 이름과 같이 말 그대로 빠르게 학습을 중단시키는 방식인데 그 기준은 training error는 계속 낮아지지만 validation error가 올라가는 지점입니다. Training set은 학습에 계속 사용 되지만 validation set은 학습에 사용되지 않기 때문에 validation error가 올라간다는 것은 모델이 overfitting되어 test set의 performance도 떨어질 것이라는 가정을 담고 있는 것이죠.   마무리  기본적인 컨셉을 잡는 강의었습니다. 구글이나 유튜브에 ‘MIT 6.S191’를 검색하시면 슬라이드와 강의 영상을 쉽게 찾으실 수 있으니 관심 있으신 분들은 꼭 본 강의를 참고 부탁드립니다!  ","categories": ["Lectures","Basic"],
        "tags": ["MIT","Deep learning","Lecture","Basic","activation function","Regulaization","overfitting","optimization"],
        "url": "http://localhost:4000/archive/Introduction-to-Deep-Learning-(MIT-6.S191)",
        "teaser":null},{
        "title": "Recurrent Neural Networks-MIT 6.S191",
        "excerpt":"Recurrent Neural Networks-MIT 6.S191  AGENDA 1. Introduction of basic concepts for RNN   움직이는 공을 찍고 있는 동영상을 일시 정지 했다고 상상해 봅시다. 멈춘 화면을 보고 다음 시점에 공이 어느 방향으로 움직일지 예측하는 것은 매우 어려운 일입니다. 하지만 만약 이전 시점들이 주어진다면 어떨까요? 공의 궤적을 통해 다음 시점에 이동할 방향을 예측하기 매우 쉬워질 것입니다. 이처럼 순서에 영향을 받는 데이터를 sequence data라고 부르며 예로는 오디오나 텍스트, 그리고 동영상등이 있습니다.   A sequence Modeling Problem: Predict the Next Word   예를 들어 다음과 같은 문장이 있다고 합시다. “This morning I took my cat for a walk.” 이 문장에서 우리는 앞 부분의 맥락을 통하여 가장 마지막 단어인 walk를 예측하고 싶습니다. 근본적으로 마지막 단어를 예측하기 위하여 우리는 단어를 vectorization할 필요가 있겠죠.   Bag of Words   가장 대표적인 vectorization 방식은 BoW로 불리는 Bag of Words 입니다. 이름 그 대로 단어들의 가방이라는 의미를 지니는데요, 문서(corpus)에 들어 있는 모든 단어의 수를 차원으로 가지는 vector에서 단어가 등장하면 1, 등장하지 않으면 0을 표기하는 방식으로 문장을 embedding(vector화) 하는 방식입니다. 예를 들어보죠. “The food was good, not bad at all.” Vs. “The food was bad, not good at all”라는 두 문장을 보면 다음과 같은 단어들이 등장합니다. [the, food, was, bad, not, good, at, all]. 여기에 보다 직관적인 이해를 위해 apple, eat이라는 단어가 다른 문장에서 등장했다고 가정 하겠습니다(따라서 등장하는 전체 단어는 [the, food, was, bad, not, good, at, all, apple, eat]이 됩니다). 이러한 상황에서 앞의 두 문장을 BoW방식으로 embedding 하면 다음과 같습니다. 첫번째 문장은 [1, 1, 1, 1, 1, 1, 1, 1, 0, 0]와 같이 되겠네요. 그렇다면 두 번째 문장은 어떨까요? 놀랍게도 두 번째 문장 역시 [1, 1, 1, 1, 1, 1, 1, 1, 0, 0]와 같은 형태를 띕니다. “The food was good, not bad at all.”와 “The food was bad, not good at all”는 정 반대의 의미를 가지는데 BoW 방식을 통해 embedding을 하게 되면 두 문장의 벡터가 정확히 동일해지는 크나큰 문제가 생깁니다.   Use a really big fixed window   또 다른 방식이 있습니다. 바로 corpus에 등장하는 모든 단어를 one-hot vector로 encoding 할 수 있지 않을까? 라는 생각에서 나온 방식인데요, 예를 들어 “This morning took the cat … “이라는 문장이 있으면 [1 0 0 0 0 (this), 0 0 0 0 1 (morning), 0 0 1 0 0 (took), 0 1 0 0 0 (the), 0 0 0 1 0 (cat), …]과 같이 vectorization을 하는 것 입니다. 그렇다면 이러한 embedding은 아무런 문제가 없을까요? 학습을 가정해 본다면 위의 문장을 보는 neural network는 무리 없이 this morning을 어떤 특정한 ‘것’으로 인지할 것입니다. 하지만 this morning이 앞 부분이 아니라, 뒷 부분에 등장하는 문장이 있다고 한다면 “This morning took the cat … “이라는 문장으로 학습되고 있는 neural network의 문장 뒤를 담당하는 parameters들은 지금까지 보지 못했던 this morning을 접해야 하기 때문에 this morning을 계속해서 이전과 같은 어떠한 ‘것’ 으로 인지하기는 어렵게 됩니다.   sequence modeling을 위하여 우리는 다음과 같은 사항을 고려해야 합니다 (input이 text라 가정하고 기술하겠습니다)      다양한 길이의 문장을 다루어야 한다   긴 길이의 문장을 다루어야 한다   단어간의 순서 정보를 보존해야 한다   sequence간의 parameters들이 공유 되어야 한다   이러한 특성을 다루기 위하여 RNN이 등장하였습니다.   RNN 구조      One to One  - 전통적인 NN -&gt; sequence를 다루는데 적합하지 않음   Many to One - 감성분석과 같은 분류   Many to Many - 매우 다양하게 활용 가능 : 본 수업의 타겟   위에 기재되어 있는 RNN을 아래 그림과 매칭시켜 보시면 이해가 편하실 것 같습니다.         Fig 1. RNN의 종류   Standard RNN gradient flow   최초에 제안된 RNN은 아래와 같이 생겼습니다.        Fig 2. Vanilla RNN 구조도   최초에 제안된 RNN을 vanilla RNN이라 부르며, cell의 연산은 아래의 식을 따릅니다:      \\(h_t = tanh(x_t\\mathbf{U}+h_{t-1}\\mathbf{W} + b_t).\\)      위 식에서 확인할 수 있듯이 vanilla RNN은 이전 state의 hidden vector와 현재 state의 input vector간의 선형 결합으로 이루어져 있습니다. 이러한 재귀적인 구조를 통해 CNN과는 다르게 데이터의 sequence 정보를 보존할 수 있지만 아래와 같은 문제점들을 지닙니다.   Exploding gradients   많은 gradients value가 1보다 큰 경우를 의미하며, 이 경우 gradient가 발산하여 어떠한 optimization도 할 수 없게 됩니다. 이러한 문제는 gradient clipping을 통해 다소 간단하게 해소 가능 합니다. Gradient clipping이란 단순하게 gradient가 계속 커지는 것을 막기 위하여 특정한 값을 곱해주어 크기를 축소시키는 방법 입니다.   Vanishing gradients   사실 이 부분이 해소가 어려워 vanilla RNN이 등장했을 때 많이 환영받지 못하였습니다. Vanishing gradients 문제는 많은 gradients value가 1보다 작은 경우 gradient가 사라지는 문제를 뜻합니다. 이를 해소하기 위하여서는 activation function을 변경하거나, 좋은 Weight 초기화 방법을 사용하는 방식이 있지만 이보다는 근본적으로 모델 구조를 변경하는 것이 가장 강건한 방식 입니다.   vanishing gradient가 왜 문제인가?   Gradient가 계속 작아지는 상황을 상상해보죠. 20단어로 이루어진 문장이 있는데 10단어째 부터 gradient가 매우 작아진다면 결국 첫 번째 단어까지 gradient 전파가 불가능해겠죠? 이런 상황에서는 RNN의 학습이 제대로 이루어질리 만무합니다. 이를 해소하기 위하여 수업에서는 아래와 같은 트릭들을 설명합니다.      Trick 1 : Activation function을 tanh에서 ReLU를 사용하는 것이 도움이 된다   Trick 2 : parameter 초기화를 identity matrix로 하면 도움이 된다   Trick 3 : LSTM, GRU와 같은 gated cell을 사용하는 것이 가장 강건한 해결책이 된다   LSTM에 대하여 알아보자!   vanilla RNN에서 발전한 형태인 LSTM에 대해서 알아보겠습니다. 우선 모델 구조는 아래와 같습니다.        Fig 3. LSTM 구조도   딱 보기에도 RNN보다 훨씬 복잡한 구조임을 확인할 수 있습니다. 수식을 통해 어떤 부분이 추가되었는지 살펴보죠.      LSTM:     \\(f_t = \\sigma(\\mathbf{W}_f\\cdot [h_{t-1},x_t]+b_f),\\)   \\(i_t = \\sigma(\\mathbf{W}_i\\cdot [h_{t-1},x_t]+b_i),\\)   \\(\\widetilde{C}_t=tanh(\\mathbf{W}_c\\cdot[h_{t-1},x_t]+b_c),\\)   \\(C_t=f_t\\times C_{t-1} + i_t \\times \\widetilde{C}_t,\\)   \\(o_t = \\sigma(\\mathbf{W}_o\\cdot [h_{t-1},x_t]+b_o),\\)   \\(h_t = o_t \\times tanh(C_t).\\)     LSTM은 기본적인 RNN이 sequence가 길어짐에 따라 발생하는 gradient descent문제를 해결하고자 등장하였습니다. LSTM은 input gate ($i_t$), output gate ($o_t$), 그리고 forget gate ($f_t$)와 같은 gate unit을 활용합니다. 각 gate들은 sigmoid layer로 구성되어 있으며 output이 1 인 경우 해당 값을 온전히 유지하게 되며, 0인 경우에는 전혀 사용하지 않습니다. 이러한 연산을 통하여 기존에 vanilla RNN에서는 할 수 없었던 sequence의 정보를 얼마나 기억할지와 잊어버릴지를 파라미터를 통하여 학습할 수 있게 된 것이죠! 사실 LSTM이 가지는 가장 큰 특성은 cell state이며 LSTM의 구조를 나타내는 Figure 3의 상단 부분 수평선이 바로 그 cell state입니다. Cell state는 gates들을 통하여 정보를 추가하거나 제거하는 역학을 맡고 있습니다. LSTM는 우선 forget gate를 통하여 버릴 정보를 선택합니다. Forget gate와 input gate는 현 시점의 input과 이전 시점의 hidden state를 받아 0과 1 사이의 output을 가지며, 앞의 두 gates들을 통하여 LSTM cell은 잊을 정보와 기억할 정보를 cell state에 update 하게 되는 것이죠. $\\widetilde{C}_t$는 최종적으로 updated된 cell state를 나타냅니다. LSTM의 output ($h_t$)은 vanilla RNN과 같이 이전 state의 hidden state value와 현재의 output을 통해 산출됩니다.   GRU 대하여 알아보자!   LSTM과 같이 vanishing gradient문제를 해결하면서도 파라미터의 수를 줄여 계산 복잡도를 감소시키기 위해 GRU model이 소개되었습니다.  GRU는 cell state와 hidden state를 결합하여 하나의 hidden state로 나타냄과 동시에 forget gate와 input gate를 결합한 update gate를 제안 합니다. Figure 4에서 확인할 수 있듯이 GRU의 구조는 LSTM에 비해 훨씬 간단합니다.        Fig 4. GRU 구조도  GRU가 가지는 cell들은 아래와 같습니다:      GRU:     \\(z_t = \\sigma(\\mathbf{W}_z\\cdot [h_{t-1},x_t]+b_z),\\)   \\(r_t = \\sigma(\\mathbf{W}_i\\cdot [h_{t-1},x_t]+b_r),\\)   \\(\\widetilde{h}_t=tanh(\\mathbf{W}_c\\cdot[h_{t-1},x_t]+b_c),\\)   \\(h_t=(1-z_t)\\times h_{t-1} + z_t \\times \\widetilde{h}_t.\\)      $z_t$는 update gate의 계산 방식입니다. GRU의 reset gate는 모델이 과거 정보의 얼마만큼을 다음 스텝으로 전달해야 하는지 결정하는데 도움을 줍니다. $r_t$가 바로 reset gate이며 이름처럼 지난 정보를 얼마나 사용하지 않을지를 결정하고 있습니다. 나머지 부분은 vinilla RNN과 LSTM에서 보신 것과 매우 흡사 합니다.   RNN application   RNN은 다음과 같이 여러 방면에서 활용되고 있습니다.      Music generation   Sentiment classification   Machine translation   마무리   사실 sentiment classification의 경우 Yoon Kim 님의 textCNN이 나온 이후로 CNN구조에 비해 크게 좋은 성능을 보이고 있지 못하고 있습니다. CNN에서도 text의 감성이나 주제를 분류하기 위하여 많은 연구들이 제안되어 왔었구요. 현재 RNN의 가장 큰 활용 분야는 역시 machine translation과 (visual)Q&amp;A가 아닐까 싶습니다. BERT와 같은 훌륭한 pretrained model도 나왔고 앞으로 계속 더 발전하지 않을까 라는 개인적인 생각이 있습니다.   사실 많은 수업에서 CNN을 RNN보다 먼저 다루고 있는 것으로 알고 있습니다만, 본 강의에서는 RNN을 2강에서, 그리고 CNN을 3강에서 다루고 있습니다. 그럼 3강에서 CNN으로 찾아뵙겠습니다.  ","categories": ["Lectures","Basic"],
        "tags": ["MIT","Deep learning","Lecture","Basic","RNN","LSTM","GRU"],
        "url": "http://localhost:4000/archive/Recurrent-Neural-Networks-(MIT-6.S191)",
        "teaser":null},{
        "title": "Deep Learning for Computer Vision-MIT 6.S191",
        "excerpt":"AGENDA 1. Introduction of basic concepts for CNN   컴퓨터는 사물을 어떻게 인식할까요? 우리가 이미지를 보는 것 처럼 똑같이 인식할 수 있을까요? 인지공학이나 뇌공학을 모르기 때문에 확신할 수는 없지만 아마 그렇지 않을 것 같습니다. 그 이유는 컴퓨터에게 이미지는 숫자들의 나열 그 이상도 그 이하도 아니기 때문이죠. 흑백 사진인 gray scaled image같은 경우는 정말 단순한 2D matrix 이며, color image의 경우에는 3D tensor가 됩니다. 아래와 같이 말이죠.         Images are numbers(Credit:MIT6.S191)   이번 강의는 이러한 이미지를 computer가 보다 잘 분류할 수 있는 Convolutional Neural Network에 대해서 소개하고 있습니다.   Tasks in Computer Vision   Vision domain에서 가장 대표적인 task는 classification입니다. 모델이 input image를 받아서 pattern을 추출한 뒤 해당 pattern이 어떤 class (category)인지를 예측하는 task를 분류라고 합니다. 이러한 분류 문제를 잘 해결하기 위해서는 모델이 각 class의 특징적인 properties를 파악하는 능력이 있어야 할 것입니다. 예를 들어 사람과 자동차를 분류하기 위해서 모델은 사람이 가지고 있는 형태 (edges), 눈, 코, 입과 같은 특징 (feature)를 잘 파악해야 하며 자동차의 edge, 창문, 바퀴과 같은 특징도 잘 파악해야 합니다.   이러한 분류 문제를 풀기 위하여 전통적으로 feature를 manually 추출 하였습니다. 물론 성능이 좋고 지금까지도 여러 분야에서 쓰이는 hand-crafted feature 방식들이 있지만 이러한 방식에서는 occlusion, deformation, illumination과 같은 이미지의 variation에 강건하지 못한 치명적인 한계가 있습니다. 다시 말해, 동일한 class임에도 불구하고 이미지의 단순한 shifting에도 분류 정확도가 매우 떨어지는 문제를 지니죠. 아래는 여러 이미지 variation에 대한 예제 입니다.         Examples of image variation types    그렇다면 이러한 한계를 어떻게 해결할 수 있을까요? 이에 대해서 연구자들이 내린 답은 classifier (model)에게 직접 중요한 feature를 추출하도록 하면 해결되지 않을까? 였습니다.   Learning Visual Features   Fully Connected Neural Network (FCN)         Fully Connected Neural Network    위의 이미지에 보이는 구조가 fully connected neural network입니다. FCN는 위에서 보이는 것처럼 모든 neuron이 연결되어 있는 구조입니다. 하지만 FCN은 input image의 위치 정보를 보존할 수 없으며 update해야할 parameter가 매우 많다는 단점이 있습니다.   그렇다면 어떻게 input의 위치 정보를 network에서 사용할 수 있을까요?   Using Spatial Structure   아이디어는 이렇습니다. FCN처럼 모든 layer의 neuron을 연결하는 것이 아니라 filter를 사용하여 특정 patch만 연결하자. 그런 다음 sliding window라는 개념을 통해서 마치 layer 전체가 연결되는 효과를 가져옵니다. 여기서 filter는 set of weights입니다. 즉 모델의 weights들의 집합인 것이죠.   Applying Filters (set of weights)      Filter는 input image 혹은 이전 layer의 local featrues를 추출하기 위하여 사용합니다.   또한 다양한 종류의 featrues를 추출하기 위하여 여러개의 filter를 사용해야 합니다.   각 filter는 weight를 공유합니다.   Feature Extraction with Convolution   만약 filter의 크기가 3x3의 matrix라면 한 filter에 9개의 weight가 있음을 의미합니다. 이 3x3 filter를 stride만큼 옮겨가면서 input image에 적용하게 됩니다.   이러한 연산을 Convolution이라 부릅니다.   아래 gif를 확인해보죠.         Convolution operation    위의 gif는 3x3 filter가 stride 2인 경우 어떻게 연산이 되는지를 보여줍니다. 순서는 다음과 같습니다. Input image에 pixel값과 filter의 weight 값이 elementwise 곱해집니다. 그 후 연산된 값들을 모두 더 해 feature map의 한 pixel값이 됩니다. Filter는 stride의 크기만큼 image (feature map)을 건너띄며 convolution 연산을 진행합니다. 첫 번째 filter가 image (feature map) 전체에 대한 연산을 마무리 하면 두 번째 filter가, 그 후에는 또 그 다음 filter가 순차적으로 convolution 연산을 진행합니다. 최종적으로 하나의 filter마다 하나의 feature map이 생성되게 됩니다.   Feature Extraction and Convolution A Case Study   그렇다면 이러한 방식은 hand-crafted feature들에 비해 장점을 가질까요?   수업에서 제시하는 예시는 다음과 같습니다. 사람은 아래 두 이미지 모두 x로 판단하겠죠.          하지만 기존의 방식으로는 이러한 variation에 강건하기 힘듭니다.하지만 우리는 shited, shrunk, rotated, deformed와 같은 variation에대해 모델이 강건하기를 바라며 이러한 이유로 convolution 연산을 사용합니다.          위의 figure와 같이 3x3 filter가 stride 1로 움직이며 연산을 하다보면 왜곡된 이미지임에도 불구하고 동일한 featrue를 강건하게 추출할 수 있게 됩니다.   Convolutional Neural Networks (CNNs)   CNN은 아래와 같은 propertise를 가집니다.      convolution : Feature map을 생성하기 위해서 학습된 weights로 이루어진 filter를 적용하여 convolution 연산을 진행   Non-linearity : Convolution 연산 이후에 비선형성을 추가해주기 위한 activation function 진행(일반적으로 ReLU-아래 figure 참조) 사용   Pooling : 각 feature map에 downsampling 진행(max나 average pooling)         ReLU function    Receptive field   Receptive field는 레이어의 각 뉴런이 입력 볼륨의에 연결된 로컬한 영역(local region)을 의미합니다. 결국 filter와 같은 의미가 되겠죠.   Stride가 1인 경우 receptive field 상 넓은 영역이 겹치게 되고, feature map의 크기도 매우 커지게 됩니다. 반대로, 큰 stride를 사용한다면 receptive field끼리 좁은 영역만 겹치게 되고 feature map의 크기도 작아지게 되겠죠.   Class Probabilities   Convolution layers와 pooling layers는 input image의 high-level features를 추출합니다.      Fully connected layer는 상기 features를 사용하여 input image의 class를 분류하는 역할을 맡습니다.   분류의 경우 softmax를 사용하여 input image가 특성 class에 속할 확률을 확률적으로 표현해 줍니다.   Backpropagation   CNN은 수 많은 filter의 weights와 fully connected layers (dense layers)의 weights를 업데이트 하기 위하여 역전파를 시행합니다.   마무리   강의에서는 CNN 구조를 분류 뿐만 아니라 segmentation, object detection, 안면 인식, 자율주행 차와 같이 다양한 분야에서 사용하고 있음을 말하고 있습니다. 물론 어느날 본질적으로 더욱 사람에 가까운, 지금 CNN이 가진 문제들을 해결해주는 brand new architecture가 등장하겠으나 현재는 또 꽤나 오랜 시간 CNN은 범용적인 domain에서 좋은 성능을 보일 것 같습니다.   수업을 듣다보니 아무래도 대상이 1학년 생들이라서 그런지 내용이 굉장히 얕더군요. 큰 그림만 잡아간다는 생각으로 편하게 보시기 좋은 포스팅이 된 것 같습니다!  ","categories": ["Lectures","Basic"],
        "tags": ["MIT","Deep learning","Lecture","Basic","CNN"],
        "url": "http://localhost:4000/archive/Deep-Learning-for-Computer-Vision-(MIT-6.S191)",
        "teaser":null},{
        "title": "Setup ubuntu for deeplearning",
        "excerpt":"본 포스터는 https://www.pytorials.com 를 참조하였습니다.   이번 포스터에서는 nvidia graphic driver &amp; CUDA를 설치하는 많은 방법들 중 개인적으로 가장 깔끔하고 사용에 문제가 없다고 생각하는 방법을 소개드릴까 합니다.   Step 0: 환경 확인   이번 포스터에서 타겟으로 삼고있는 OS는 ubuntu 18.04 이며 설치하고자 하는 CUDA version은 10.0 입니다. CUDA의 경우 사용하시는 deeplearning library에 맞추어 필요한 버전을 설치해 주시면 되겠습니다.   Step 1: Update &amp; upgrade a system   우선, system update와 upgrade를 진행합니다.   sudo apt-get update &amp;&amp; sudo apt-get upgrade -y   Step 2: Install Dependencies   이후 필수적인 dependencies를 설치하여 줍니다.   sudo apt-get install build-essential sudo apt-get install cmake git unzip zip sudo apt-get install python-dev python3-dev python-pip python3-pip   Step 3: Install linux kernel header   uname -r   위의 line을 통하여 linux kernel version을 얻을 수 있습니다. 저는 ‘4.15.0-76-generic’ 이네요.   사용하는 linux kernel에 맞는 linux header를 설치하기 위하여 아래와같이 명령어를 입력해 주시면 됩니다:   sudo apt-get install linux-headers-$(uname -r)   Step 4: Install NVIDIA CUDA 10.0   깔끔한 설치를 위해 기존에 설치되어있던 nvidia driver를 삭제하여줍니다. (처음 설치하시는 분들은 생략 가능합니다.)   sudo apt-get purge nvidia*  sudo apt-get autoremove  sudo apt-get autoclean  sudo rm -rf /usr/local/cuda*   CUDA 설치는 아래와같이 이루어집니다:   sudo apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub echo \"deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64 /\" | sudo tee /etc/apt/sources.list.d/cuda.list sudo apt-get update sudo apt-get -o Dpkg::Options::=\"--force-overwrite\" install cuda-10-0 cuda-drivers   Step 5: Reboot the system   sudo init 6   Step 6: Set environment path   .bashrc 파일에 path를 잡아준 후 source 명령어를 통해 재시행 시켜줍니다. 이후 nvidia-smi를 통하여 나의 GPU들이 잘 잡혀있는지, driver는 잘 설치되었는지 확인합니다.   echo 'export PATH=/usr/local/cuda-10.0/bin${PATH:+:${PATH}}' &gt;&gt; ~/.bashrc echo 'export LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}' &gt;&gt; ~/.bashrc  source ~/.bashrc sudo ldconfig nvidia-smi   Step 7: Install cuDNN and NCCL   마지막 step에서는 cudnn과 nccl을 설치해 보겠습니다.   cudnn은 gpu를 보다 잘 사용할 수 있게 도와주는 library라고 생각해 주시면 되시며, NVIDIA Collective Communications Library (nccl)는 multi-gpu나 multi-node의 최적화를 시켜주는 library 라고 생각해 주시면 됩니다.   먼저 cudnn 설치부터 알아보겠습니다. 설치를 위해 Nvidia/cudnn 에 접근하여 cuDNN Download &gt; log-in &gt; Download cuDNN v7.6.5 (November 5th, 2019), for CUDA 10.0 &gt; cuDNN Library for Linux 의 순서를 따라주시면 ‘cudnn-10.0-linux-x64-v7.6.5.32.tgz’ 파일을 다운받을 수 있습니다. 만약 다른 버전의 CUDA를 위한 cuDNN을 설치하시는 분이시라면, 가지고계신 CUDA version에 맞는 cuDNN을 다운받아 주세요!   다운 받은 파일을 설치할 차례입니다:   tar -xf cudnn-10.0-linux-x64-v7.6.5.32.tgz  sudo cp -R cuda/include/* /usr/local/cuda-10.0/include sudo cp -R cuda/lib64/* /usr/local/cuda-10.0/lib64   끝으로 NCCL 설치를 해보겠습니다. 설치를 위해 Nvidia/nccl 에 접근하시어 log-in &gt; Download NCCL v2.5.6, for CUDA 10.0, Nov 19,2019 &gt; O/S agnostic local installer 의 순서를 따라주시면 ‘nccl_2.5.6-1+cuda10.0_x86_64.txz’ 파일을 다운 받을 수 있습니다. 마찬가지로, 다른 버전의 CUDA를 가지고 계신 분은 해당 버전에 받는 nccl을 받아주시기 바랍니다.   마지막 명령어 입니다:   tar -xf nccl_2.5.6-1+cuda10.0_x86_64.txz  cd nccl_2.5.6-1+cuda10.0_x86_64  sudo cp -R * /usr/local/cuda-10.0/targets/x86_64-linux/  sudo ldconfig   마무리   이로써 deep learning을 위한 nvidia driver 설치와 부수적인 library 설치를 마쳤습니다. 모든 분들이 성공적으로 설치를 마치셨기를 바라면서 글을 마무리 짓겠습니다.  ","categories": ["Ubuntu"],
        "tags": ["ubuntu","nvidia","cuda","cudnn"],
        "url": "http://localhost:4000/archive/Set-up-ubuntu-for-deeplearning",
        "teaser":null},{
        "title": "Rotate display on terminal",
        "excerpt":"이번 포스터에서는 terminal에서 display를 회전하는 방법에 대하여 살표보겠습니다.   저는 보조 모니터를 세로형으로 사용하고 있습니다. 이런 상황에서 vnc viewer와 같이 원격 viewer를 사용할 때 세로형 모니터를 Landscape (정방향)으로 돌려주지 않으면 매우 불편한 상황들이 많기 때문에 매우 유용하게 사용하는 명령어 중 하나 입니다.   우선 xrandr 명령어를 통해 어떤 port (DP-0, DP-1, HDMI-0, and etc)에 screen이 있는지 확인할 수 있습니다.      저의 경우에는 HDMI-0과 DP-2에 연결이 되어 있네요. HDMI-0을 회전한 후 정렬까지 해보겠습니다:   xrandr --output HDMI-0 --rotate normal xrandr --output DP-2 --pos 3840x0   이후 다시 세로형 모니터로 전환하기 위해 아래의 명령어를 입력하시면 됩니다:   xrandr --output HDMI-0 --rotate left xrandr --output DP-2 --pos 2160x800   position같은 경우에는 해상도 별로 둬야하는 위치가 달라지기 때문에 각자 상황에 맞게 변경해 주시면 되겠습니다!   ","categories": ["Ubuntu","Remote"],
        "tags": ["ubuntu","xrandr","display","rotate"],
        "url": "http://localhost:4000/archive/rotate-display-on-terminal",
        "teaser":null},{
        "title": "Control git on terminal",
        "excerpt":"이번 포스터에서는 github repository와 local folder를 연동하여 repository를 관리하는 방법에 대하여 알아보겠습니다.   Git 설치   우선 아래 명령어를 통하여 git을 설치하겠습니다.   sudo apt update sudo apt-get install git   이후 버전 확인은 아래와같이 할 수 있습니다.   git --version   Local folder 연동   git을 설치하셨다면 다음 step은 git repository와 local folder를 연동하는 것 입니다. (git repository는 생성되어 있다고 가정합니다)   # local folder로 이동 cd /path/to/local/folder  # git 시작 git init git remote add origin your/repository/url git remote -v   Repository에 push하기   github repository에 push, 즉 sync되어 있는 repository를 update 하기 위해서는 아래 세 가지 step이 필요합니다.      add   commit   push   간략히 살펴보자면 아래와 같습니다.      add            Repository를 update 하기 위한 파일을 선택합니다           commit            git을 사용하는 이유라고 생각해도 무방한 version을 생성하는 명령어 입니다           push            선택한 파일들을 생성한 version으로 update 합니다           사용법은 아래와 같습니다   # 모든 파일 add git add .  # \"First update\" 라는 이름으로 commit 진행 git commit -m \"First update\"  # push git push origin master   git status   하지만 항상 모든 파일을 git add . 으로 add를 한다면 원하지 않는 파일을 add 하거나, 변경된 모든 파일을 하나로 commit 할 수 밖에 없는 단점이 있습니다.   이러한 상황에서 유용하게 사용할 수 있는 명령어가 git status 입니다.   git status   git sataus를 입력하시면 어떤 파일들에 수정이 있었는지(modified files) 쉽게 알 수 있습니다. 그럼 원하는 파일만 아래와 같이 add 할 수 있겠죠.   git add file1 file2   뿐만 아니라 새로운 파일인 untracked files에 대한 정보도 확인 가능합니다.   항상 무시할 파일 지정   github에는 40M 이상 용량을 차지하는 파일을 올릴 수 없습니다. 그럼 폴더에 데이터가 있으면 어쩌죠? 혹은 .idea나 cache file들 처럼 원하지 않으나 생성되는, 그래서 지저분해 보이는 파일들을 항상 수동으로 관리 해야 할까요?   이와같이 코드에는 필요하나 repository에는 ‘항상’ add 하지 않을 파일들을 지정할 수 있습니다.   .gitignore 파일로 말이죠!   # open .gitignore using your favorite editor vim .gitignore   이후 아래와같이 .gitignore 파일을 원하시는대로 수정해 주시면 됩니다.   ## Ignore files ##  .idea path/to/large/file   만약 commit을 진행 한 후 .gitignore file에 내용을 수정하셨다면 git add . git commit git push를 하셔도 지정된 파일이 repository에서 삭제되지 않음을 볼 수 있습니다.   이런 경우에는 아래의 명령어를 입력해 주시면 해결 가능 합니다.   git rm -r --cached . # remove cache  git add . git commit -m \"Update .gitignore\" git push origin master   마무리   이번 포스터에서는 git에 관련된 매우 필수적인, 그리고 유용한 명령어들을 살펴보았습니다.   그럼 다음 포스터로 뵙겠습니다.  ","categories": ["Git","Ubuntu"],
        "tags": ["ubuntu","git"],
        "url": "http://localhost:4000/archive/control-git-on-terminal",
        "teaser":null},{
        "title": "conda 가상환경 사용하기",
        "excerpt":"이번 포스터에서는 python virtual environment, 아나콘다를 활용한 파이썬 가상환경 구축에 대하여 알아보겠습니다.   Anaconda   Linux 계열에는 기본적으로 파이썬이 설치되어 있습니다 (물론 2.x 지만요..).   물론 추가적으로 python3 를 설치하고 pip3를 통하여 package들을 설치 할 수 있습니다. 또한 기본 python을 사용하여도 가상 환경 구축이 가능 합니다.   하지만 가상환경을 사용하는 경우에 anaconda의 가상환경을 사용하는것이 시스템 파이썬의 가상환경을 사용하는 것에 비해 매우 직관적이고 편하기 때문에 Anaconda의 인기가 높은 것 같습니다.   가상환경   그렇다면, 우리는 왜 가상환경을 사용해야 할까요? 그냥 OS-based python을 그대로 사용하면 안 될까요?   제가 내린 답은 코드 공유에 있습니다.   우리는 자신의 코드를 자신만 사용하는 환경에 있지 않습니다. 다시 말해서 나의 코드를 다른 사람이 그 사람의 환경에서 돌리는 상황도 있으며, 다른 사람의 코드를 나의 환경에서 돌리는 경우도 있습니다.   실례로 github은 전 세계적으로 매우 활발한 코드 저장소 (물론 버전 관리가 큰 목적이지만요,)이며 나의 코드를 github에 활발히 공유하기도하며, 다른 사람의 좋은 코드를 clone하여 나의 환경에서 사용하기도 합니다.   이러한 상황에서 base 환경만 사용한다면 어떤 문제가 발생할까요?   답은 바로 package들 사이의 의존성에 있습니다. 나의 base 환경에는 pytorch version 1.4가 설치되어 있으며, github에 올라와 있는 내가 사용하고 싶은 코드는 pytorch version 0.4로 짜여있다고 가정해보죠. pytorch verion 1.4와 0.4는 요구하는 numpy version이 다릅니다. 이러한 상황에서 base environment만 사용하고 있다면 각 코드를 실행시킬때마다 pytorch와 numpy의 버전 업그레이드 (or 다운그레이드)를 해주어야겠죠. 이러한 package가 하나라면 해볼만 하겠지만 여러 dependency가 복잡하게 얽힌다면.. 상상만해도 화가 날것만 같습니다.   이런 문제를 해결해주는 툴이 가상환경입니다. 각 python project별로 가상환경을 만들어 해당 Project에 맞는 패키지 버전들을 설치한다면 위의 문제를 겪지 않고 깔끔하게 환경을 관리할 수 있습니다.   Anaconda 설치하기   설치파일을 받는 방법에는 두 가지가 있습니다:           wget 사용        cd /path/to/download/folder   # Install miniconda  wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh   # Install anaconda  wget https://repo.anaconda.com/archive/Anaconda3-2019.10-Linux-x86_64.sh                직접 다운       Anaconda Link       Anaconda에는 anaconda (full-version)과 miniconda (lite-version) 이 있습니다. 저희는 가상환경을 구축한 후 필요한 패키지들을 어차피 추가할 예정이며, full-version의 모든 패키지들을 사용할 경우가 별로 없기 때문에 개인적으로는 miniconda를 설치하셔도 아무 불편함이 없지않을까 생각합니다.   설치 파일을 받은 후에,   # +x 명령어를 통하여 실행 권한 부여. Miniconda까지 입력하신 후 # tab 키를 누르시면 다운 받으신 버전에 맞추어 자동 완성 되십니다. sudo chmod +x ./Miniconda~~.sh  # Install miniconda ./Miniconda~~.sh   가상환경 사용하기   conda를 설치하셨으니 지금부터 conda를 활용하는 방법에 대하여 알아보겠습니다.   가상환경 생성   conda를 설치하셨으면 가상환경을 생성이 역시 첫 번째 테스크가 될 것 같습니다. 가상환경 생성은 아래와 같이 할 수 있습니다:   # python version 3.6을 사용하는 가상환경 생성 conda create -n env_name python=3.6  # python version 3.7을 사용하며, 필요한 Package들이 #  requirements.txt에 정리되어 있는 경우 가상환경 생성 conda create -n env_name python=3.7 requirements.txt   위의 라인에서 requirements.txt는 현재 shell을 실행하는 디렉토리에 존재해야 합니다.   env_name은 typing 하기 쉽고, 직관적인 이름으로 지으시기를 추천 드립니다. 논문의 nickname (fgsm, gan, wgan-gp, …)으로 짓는 것이 가장 편한데 논문 reproduce가 아닌 경우 간편하고 식별력이 좋은 이름이 좋습니다.   가상환경 활성화/비활성화   생성된 가상환경에 들어가고 다시 base environment로 나오는 방법은 아래의 간단한 라인으로 실행 가능합니다:   # 가상환경 활성화 conda activate env_name  # 가상환경 비활성화 conda deactivate   이후 활성화된 가상환경에 원하는 패키지들을 설치하기 위하여서는 base 환경과 마찬가지로 pip install package 또는 conda install package를 사용할 수 있습니다.   사용하고 있는 가상환경 export하기   참 유용하고 편리한 기능인 export에 대하여 알아보겠습니다.   conda env export &gt; file_name.yml 을 통하여 현재 활성화된 가상환경에 설치된 패키지들의 버전 정보를 담은 파일(.yml)을 export할 수 있습니다. 이 기능이 좋은 이유는 역시 코드 공유에 있습니다. 나의 개발 환경을 그대로 저장하여 코드와 함께 전달해 준다면 본 코드를 받은 공동 작업자는 힘들이지 않고 같은 환경의 가상환경을 쉽게 만들어 충돌 없이 코드를 시행할 수 있습니다. 마찬가지로 github에 upload를 할 때에도 해당 .yml 파일을 함께 올려주면 upload한 코드를 사용하는 사람들이 정말 고마워할것 입니다.   그렇다면, 다른 사람이 export한 가상환경 파일 (.yml)을 사용하여 나의 환경에서 가상환경을 어떻게 구축할 수 있을까요? 이는 아래 라인과 같이 매우 간단하게 진행할 수 있습니다:   cd path/to/where/yml/file/exist conda env create -f file_name.yml -n env_name   위의 라인에서 file_name은 다른 사람이 export한 yml 파일의 이름을 의미하며, env_name은 내가 구축하려는 가상환경의 이름을 의미합니다.   가상환경 확인 및 삭제   끝으로 존재하는 가상환경을 확인하고 또 삭제하는 방법에 대하여 알아보겠습니다.   # 가상환경 확인 conda env list  # 가상환경 삭제 conda env remove --name env_name   마무리   본 포스터에서는 가상환경이 왜 유용한지에 대하여 간략한 소개와 함께 아나콘다를 소개해 드렸습니다. 아나콘다를 활용하여 가상환경을 구축하는 것은 나의 base 환경을 깨끗하게 유지해주는 것 뿐만 아니라, 협업의 관점에서 매우 활용도가 높습니다.   모두 conda를 활용한 가상환경 구축 및 사용이 익숙해지는데 본 포스터가 조금이나마 도움이 되셨길 바랍니다.  ","categories": ["Remote","Ubuntu"],
        "tags": ["ubuntu","remote","conda","python","environment","가상환경"],
        "url": "http://localhost:4000/archive/conda",
        "teaser":null},{
        "title": "SSH on Ubuntu 18.04",
        "excerpt":"이번 포스터에서는 원격 제어의 꽃인 Secure Shell Protocol (SSH)의 사용법에 대하여 알아보겠습니다.   SSH   SSH는 FTP나 Telnet에 비해 보안성이 좋은 통신 프로토콜 입니다.   SSH 설치   Ubuntu 18.04에는 SSH client가 default로 설치되어 있습니다. SSH server 설치를 위하여 아래와 같이 간단한 명령어를 입력하시면 됩니다.   sudo apt-get install openssh-server   설치후에 SSH는 자동 실행 됩니다.   방화벽이 실행중이라면 아래의 명령어로 SSH port를 추가하여 줍니다.   sudo ufw allow ssh   SSH 사용하기   Ubuntu에 SHH를 설치하고나면, 아래와같은 명령어를 통해 local machine에서 ubuntu로 접근이 가능합니다.   # 방법1 ssh ubuntu_name@remote_ip  # 방법2 ssh -l ubuntu_name remote_ip   위의 두 방법 중 편하신 방법으로 사용하시면 되며, ubuntu_name은 ubuntu에서 terminal을 여시면 @앞에 있는 부분입니다. 본인이 사용하고 있는 ip 주소는 ifconfig 명령어를 통해 쉽게 파악할 수 있습니다.   위의 명령어를 통하여 ubuntu에 접근 한 후, ubuntu 계정 비밀번호를 입력하시면 원격 접속이 가능합니다.   RSA key 오류   SSH를 사용하여 원격 접속을 하다보면 가끔 아래와같은 오류를 마주할때가 있습니다:   WARNING: REMOTE HOST IDENRIFICATION HAS CHANGED!   이럴때는 당황하지 마시고 아래의 명령어를 통하여 해결하도록 합시다.   ssh-keygen -R remote_ip   위의 명령어를 통하여 원격 machine에 접속한 후, ‘service sshd restart’ 를 입력해 주시면 오류를 해결할 수 있습니다.   마무리   매우 간단히 몰라서는 안 될 ssh에 대하여 알아봤습니다.   유용하게 사용되길 바랍니다.  ","categories": ["Remote","Ubuntu"],
        "tags": ["ubuntu","ssh"],
        "url": "http://localhost:4000/archive/ssh",
        "teaser":null},{
        "title": "About GANs and Adversarial Training",
        "excerpt":"AGENDA 1. GANs 2. Adversarial examples 3. Adversarial training   사실 본 포스트는 논문에 관한 이야기는 아닙니다. 하지만 지인이나 연구실 동료들과 함께 이야기를 하면서 adversarial training에 대하여 잘못 이해하고 있는 부분들이 있어 짧게 짚고 넘어가고자 합니다.   Ian Goodfellow   개인적으로 Ian Goodfellow를 참 좋아합니다. 그의 논문을 보고 있으면, 연구자로서 저런 생각을 할 수 있으면 얼마나 좋을까 라는 생각을 하지 않을 수 없습니다. 그의 연구 중 역시나 가장 대표 되는 업적은 GAN이 아닐까 싶습니다. 본 포스트의 목적과는 거리가 있지만, Ian 이야기를 한 김에 GAN에 대한 저의 생각을 짧게 말씀 드리려 합니다.   지금부터의 내용들은 순전히 저의 뇌피셜임을 말씀 드립니다             Fig 1. GAN   2014년 NIPS에 발표된 Generative Adversarial Nets은 deep learning에 한 획을 그은 연구였습니다. Nips 2016 Tutorial에서 Ian은 화폐 위조범과 경찰을 예로 들며 Generator와 Discriminator를 설명합니다. 위의 그림과 같이요. 실질적인 설명은 다들 너무나도 잘 아실테니 넘어가겠습니다.   우선, 드리고 싶은 질문이 있습니다:     GAN이 없다면 Deep learning은 어떤 역할을 하고 있을까?    현재 Deep learning의 task는 크게 분류와 생성으로 구분 지을 수 있을 것 같습니다. 여기서 생성이 빠진다면 DNN은 여전히 단순한 분류 작업만 하고 있을지도 모르겠습니다. 이처럼 GAN은 Yann LeCun이 극찬 했던 것 처럼 정말 대단하고도 참신한 아이디어 입니다.   그렇다면, GAN은 정말 ‘무’ 에서 창조된 ‘유’ 였을까요? 저의 생각은 ‘그렇지 않다’ 입니다.             Fig 2. GAN model   위의 이미지는 통상적으로 우리가 그리는 GAN의 형태를 띄고 있습니다. 어디서 많이 보신 구조가 들어 있지 않으신가요? 네 맞습니다. 아래 보이시는 Auto encoder와 매우 흡사한 구조를 가지고 있죠.             Fig 3. Auto encoder   아래 이미지에서 확인 할 수 있듯이 GANs의 Generator는 AE의 Decoder와, 그리고 Discriminator는 Encoder와 각각 맵핑됨을 알 수 있습니다.   여기서 중요한 점은 AE의 decoder는 GANs의 Generator와 같이 인풋을 받아 이미지를 생성하는 역할을 하지만 Generator와는 다르게 매우 예쁜 encoder가 잘 생성해준 latent vector를 받는다는 사실 입니다. 그에반해, Generator의 경우에는 모델의 최전선에 있기 때문에 누군가 만들어준 예쁜 인풋을 받을 수 없죠. 그래서 고민 끝에 나온 것이 z ~ N(0, I)인 vector를 무수히 많이 넣어 z를 latent vector로 만들자는 아이디어가 아니었을까 싶습니다. 실제로 이 z space를 latent space, z를 latent code라 부르고 있으며 이 space 를 잘 학습된 GAN을 통하여 역으로 추정하는 연구가 최근 Bolei Zhou 교수님을 필두로 매우 활발하게 진행되고 있습니다. 매우 재미있는 연구 분야니 GAN에 관심이 있으시면 꼭 찾아 보시기를 추천 드립니다.   제가 좋아하는 Ian Goodfellow 이야기가 나와서 신나서 지금까지 떠들었네요.   그럼 지금부터는 본 포스트의 목적인 Adversarial Training에 대하여 말씀 드리겠습니다.   Adversarial Example  Adversarial training을 제대로 이해하기 위해서는 adverasrial example에 대한 이해하 우선되어야 합니다. Adversarial example을 이야기 할 때 빠질 수 없는 논문 “Explaining and Harnessing Adversarial Examples”에서 본따 만든 아래 그림을 보시면 직관적인 이해가 편할 것 같습니다.             Fig 4. Adversarial example   Figure 4에서 보여주고 있는 내용은, 우리가 보고 있는 real world data (여기서는 이미지)에 사람이 구분할 수 없는 noise (가운데 perturbation 부분)을 더해주면 최종적으로 사람은 noise를 더하기 전과 다름을 느낄 수 없으나 기계는 이를 다른 class로 예측 한다는 것 입니다. 위의 Figure를 살펴보면, 가장 왼쪽에 귀여운 고양이를 사람이나 분류기(여기서 가정은 이미 학습을 완료한 분류기 입니다.)나 모두 고양이로 인식할 것 입니다. 이 고양이 이미지에 가운데 있는 perturbation이라 불리는 noise를 더해주고 나면 가장 오른쪽 고양이가 나오는데 사람이 보기에는 여전히 너무나도 귀여운 고양이지만, 분류기가 보기에는 바나나로 보이게 됩니다. 이와같이, 사람에게는 전혀 인식되지 않으나 분류기에게는 치명적인 차이점을 제공하는 noise를 perturbation이라고 부르며 가장 오른쪽에 있는 이미지처럼 사람이 보기에는 정상적인 데이터와 다름이 없으나 기계가 보기에는 전혀 달라져버린 데이터를 adversarial example이라 부릅니다. 이러한 adversarial example을 비단 이미지뿐만 아니라 텍스트, 음성, 그래프와같이 우리가 생각하는 모든 input에서 생성될 수 있습니다.   그럼 위의 Perturbation은 어떤 식으로 생성이 되는 것일까요? 가장 대표적인 gradient-based 방식으로 살펴보겠습니다.             Fig 5. CNN Training   우선 우리가 익히 알고있는 것처럼, 모델의 weights는 input을 모델에 feeding한 후 모델의 예측값과 실제 레이블 사이의 오차를 바탕으로 backpropagation을 진행하며 update되게 됩니다. 이러한 반복적인 과정을 거치고 나면 loss가 수렴하며 학습이 완료되죠.             Fig 6. CNN Fine-tuning   비슷한 컨셉으로 모델의 특정 부분, Figure 6에서는 dense layer (classifier) 이전 부분의 weights를 freeze한 후 backpropagation을 진행하여 freeze가 되지 않은 부분만 학습하는 기법을 finetuning이라 부릅니다. 그렇다면 전체 모델을 freeze하면 어떤 일이 발생할까요? 아래 Figure처럼 말이죠.             Fig 7. Create perturbation   모델 전체를 freeze하면 대체 loss를 어디에 전파해야하지? 라는 생각이 드실것 같습니다. 그에대한 해답은 Fig 4에 있는데요, 이전 포스팅에서 설명드렸던 FGSM에서 perturbation을 구하는 목적식은 $sign(\\nabla_xJ(\\theta, x, y))$ 입니다. 여기서 $J$는 target classifier (분류기)의 목적식이 됩니다. 살펴보면 결국 $\\theta$와 $x$를 통해 $\\hat{y}$을 구한 후 $y$와의 비교를 통해 gradients를 구해갈텐데 위의 식을 잘 살펴보시면 그 대상이 $x$임을 알 수 있습니다. 다시 말해, 모델 전체의 weights를 fix시켜두고 loss에 대한 gradients를 input 단에서 구하는거죠. 마치 layer의 weights를 update시키는 것 처럼요!   약간 다른점은 일반적으로 weights를 업데이트 할 때에는 gradient에 learning rate를 곱해주는 방식으로 진행 되지만 FGSM에서는 gradient의 방향만 따온 후 거기에 $\\epsilon$이라는 pyher-parameter를 곱해주어 perturbation을 구해준다는 것입니다.   지금까지 gradient-based 방식에서 perturbation을 어떻게 구하는지 살펴 보았습니다. 지금부터는 제가 본 포스트에서 최종적으로 하고싶었던 말씀을 드려보려 합니다.   Adversarial Training  우선 adversarial training에 대하여 간략하게 설명을 드리겠습니다.             Fig 8. Adversarial Training   Figure 8에서 윗 부분만 살펴보면 일반적인 DNN을 학습시키는 과정과 동일함을 확인할 수 있습니다. Input을 feeding하고 모델이 결과를 산출하면 그 결과를 통해서 해당 모델을 업데이트 하는 방식 말이죠. Adversarial training은 이러한 일반적인 학습 중에 한 가지 트릭을 더해줍니다. 바로 학습을 진행하면서 내부적으로 adversarial example을 생성하고, 생성된 example을 다시 모델에 feeding함으로써 모델이 adversarial example에 robust할 수 있는 특릭 말이죠. FGSM을 예로 들자면, 모델을 구축하는 사람은 이미 FGSM의 목적식을 알고 있습니다. 우리 모두가 알고있죠! 그 목적식을 모델이 학습되는 중에 해당 모델에 적용시켜 매 시점 feeding되는 input에 맞는 adversarial example을 생성합니다. 순서를 살펴보자면 아래와 같겠죠:     Normal input을 model에 feeding 한다   1 에서 산출된 예측값과 실제값을 통하여 모델을 업데이트 한다   1 에서 넣은 input에 대한 adversarial example을 2 에서 업데이트 된 모델을 통하여 생성한다.   3 에서 생성된 adversarial example을 2 의 모델에 다시 feeding한다   1 - 4 를 반복한다   Adversarial training의 기대 효과는 adversarial example에 robust한 모델 구축 입니다. 이러한 기대가 가능한 이유는 3번에서 들어가는 adversarial example에 대한 $hat{y}$는 매우 높은 확률로 잘못 되어 있지만, 이를 실제 $y$를 제공하고 모델을 업데이트 하는 과정에서 모델이 스스로 학습해 나가기 때문이죠. 실제로 여러 공격을 받고 한계를 보이고 있으나 adversarial training이 매우 직관적이고 특정한 상황 내에서는 매우 효과적인 방어 기법임은 틀림이 없습니다.   Misconception about Adversarial Training  그렇다면 제가 지금까지 저의 주변에서 가장 많이 봤던 adversarial training에 대한 오해는 무엇일까요? 바로 Robustness에서 오는 오해였습니다. 모델이 adversarial example에 robust하기를 기대하는 adversarial training을 마치 real data에 robust한 모델을 만드는 것으로 착각하는 것이죠. 우리가 real data에 robust한 모델을 만들기 위해서 흔히 “Data Augmentation”을 진행합니다. 다시 말해, 제가 가장 흔하게 본 오해는 adversarial training 내부에서 생성되는 adversarial example들을 마치 input에 대한 augmentation 기법으로 이해하는 것 이었습니다. 이런 이해가 왜 오해인지 간단한 실험을 통해 살펴보겠습니다. 실험은 CIFAR10에 대한 분류 task이며 사용한 model은 ResNet18 (custom)입니다. 모든 학습은 200 epochs 진행하였습니다.             Fig 9. Data Augmentation   우선 데이터 augmentation의 효과에 대해 살펴보면 Figure 9와 같습니다. X축에 대한 설명은 아래와 같습니다:     A: W/O augmentation   B: Random crop   C: Random horizontal flip   D: Random rotation (15)   E: ALL + SGD (optimizer)   F: ALL + Adam (optimizer)   우선 A-E를 살펴 보면 data augmentation을 사용하지 않은 경우보다 사용한 경우 확실히 성능 향상이 있음을 볼 수 있습니다. E와 F가 조금 더 흥미로운 결과를 보여주는데 E는 나열된 모든 augmentation 기법을 사용한 후 optimizer로 SGD를 사용한 결과이며, F는 E와 같지만 optimizer로 Adam을 사용한 결과입니다. 이를 볼때 optimizer의 선택도 성능에 매우 중요한 역할을 한다는 것을 알 수 있습니다. 그렇다면 adversarial training도 이러한 결과를 보여줄까요?             Fig 10. Adversarial Training   Figure 10을 보면 그렇지 않다는 것을 알 수 있습니다. 성능이 소폭 상승하기는 하였으나 Figure 9의 data augmentation 기법들과 같은 드라마틱한 차이는 없습니다. x축에 대한 설명은 아래와 같습니다:     A: W/O augmentation   B: 모델 학습 시작과 함께 adversarial training 진행   C: 모델 학습이 어느정도 수렴한 후 adversarial training 진행   D: 모델 학습을 완료한 후 dense layer에 대하여 fine-tuning을 adversarial training 으로 진행             Fig 11. Adversarial Training   Figure 11에서는 모든 결과를 비교하고 있습니다. 이렇게 보면 그 차이가 얼마나 뚜렷한지를 알 수 있습니다. 결국 adversarial training은 data augmentation 효과가 없음을 알 수 있습니다.   마무리   주변에서 본 adversarial training에 대한 오해를 풀기위해 작성한 글입니다. Adversarial training으로 가기위하여 adverarial example에 대하여 간략하게 살펴보았으며, 제가 좋아하는 연구자가 등장하여 그의 대표 연구인 GAN에 대하여 저의 생각도 짧게 풀어보았습니다.   결론적으로 제가 주변에서 가장 많이 보았던 adversarial training에 대한 오해는 이 기법이 비단 adversarial example에 robust할 뿐만 아니라 real world data에도 robust할 것이다 였습니다. 다시 말해, data augmentation의 하나의 갈래라고 생각하는 분들이 정말 많았습니다. 본 포스트의 마지막 section에서 간단한 실험을 통하여 이러한 생각이 왜 오해가 되는지를 살펴보았습니다.  ","categories": ["Adversarial example"],
        "tags": ["Adversarial example","adversarial training","Adversarial defense","Adversarial defend","Ian Goodfellow","GANs","GAN"],
        "url": "http://localhost:4000/archive/ADV_Training",
        "teaser":null},{
        "title": "Docker란 무엇인가",
        "excerpt":"진행 환경 - Ubuntu 18.04 - docker version: 19.03 - CUDA &amp; cuDNN 설치 가정   연구실에서 서버 및 도커를 사용할 기회를 주신 지도교수님께 무한한 감사를 드립니다 (DSBA lab)   Docker를 설차히고 사용하기 이전에 docker가 대체 무엇인지 간략하게 알아봅시다.   Docker            Fig 1. docker logo   Docker는 서버를 잘 관리하기 위해 개발된 플랫폼입니다.   개인적으로 큰 서버를 관리 및 사용을 해본적은 없지만 직관적으로 생각해도 OS부터 개발 환경까지 매우 다양한 변동성을 모두 관리하고 적응하여 사용하는 것은 큰 문제같습니다.   도커는 이러한 어려움을 완화하려 등장하였으며, Container의 사용으로 이를 구체화 하고 있습니다. 위의 이미지에서 고래를 Docker, 그 위에 쌓여 있는 것들을 Container라 부릅니다.   Container  Container는 프로세스를 실행하는 독립적인 공간입니다.   위의 문장을 보시면 아마 VirtualBox와 같은 가상머신이 떠오르실거라 생각됩니다. Docker가 기존의 가상머신과 다른점은 OS를 가상화 하지 않는다는 점입니다. OS를 가상화하여 Host OS위에 새로운 OS를 설치하는 기존의 방식은 매우 느리고 무거웠죠. 하지만 Docker에서는 container라는 개념을 사용하여 사용자에게 Host OS위에 독립적인 공간들을 제공합니다. 하나의 서버에서 여러개의 container를 실행시키더라도 서로 영향을 미치지 않고 독립적으로 실행됩니다.   Image  Docker에서 container와 함께 반드시 이해해야 하는 개념이 Image 입니다.   Image의 경우 Host OS위에서 내가 사용할 환경(container에서 사용할 설정값)을 구축한 말 그대로 이미지입니다. 앞서 말씀드린 container는 image위에 올라가게 되는데 기본적으로 image에서 설치한 library들을 가지고 있습니다. 개별 container에서는 image에서 가지고 있지 않는 환경을 추가적으로 구축할 수 있습니다.   아마 지금 와닿지 않는 개념들은 이후 설치를 알려드리는 포스트를 보시면 조금 더 이해가 쉬울 것 같습니다!  ","categories": ["Docker"],
        "tags": ["ubuntu","Docker"],
        "url": "http://localhost:4000/archive/docker1",
        "teaser":null},{
        "title": "Deeplearning을 위한 Docker 설치",
        "excerpt":"진행 환경 - Ubuntu 18.04 - docker version: 19.03 - CUDA &amp; cuDNN 설치 가정   연구실에서 서버 및 도커를 사용할 기회를 주신 지도교수님께 무한한 감사를 드립니다 (DSBA lab)   Docker와 Nvidia docker 설치하기   Install Docker  OS를 설치한 후 GPU에 맞는 CUDA와 cuDNN만을 설치한 후 Docker 설치를 진행합니다.      기존 도커 제거(있는 경우 진행)     sudo apt-get remove docker docker-engine docker.io containerd runc           라이브러리 업데이트 및 설치     sudo apt-get update &amp;&amp; sudo apt-get upgrade -y sudo apt-get install \\   apt-transport-https \\   ca-certificates \\   curl \\   gnupg-agent \\   software-properties-common           도커 공식 GPC key 추가     curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -           도커 설치     sudo add-apt-repository \\  \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\  $(lsb_release -cs) \\  stable\" sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io           Install Nvidia Docker  Docker version  distribution=$(. /etc/os-release;echo $ID$VERSION_ID) curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list  sudo apt-get update &amp;&amp; sudo apt-get install -y nvidia-container-toolkit sudo systemctl restart docker   Nvidia docker의 경우 최신 버전의 도커(2020 이후, docker version 19.03 이후)를 사용하시는 경우 설치 하지 않으셔도 GPU 자원을 사용하실 수 있습니다.  ","categories": ["Docker"],
        "tags": ["ubuntu","Docker"],
        "url": "http://localhost:4000/archive/install-docker",
        "teaser":null},{
        "title": "Deeplearning을 위한 Docker 사용하기",
        "excerpt":"진행 환경 - Ubuntu 18.04 - docker version: 19.03 - CUDA 10.2 &amp; cuDNN 설치    연구실에서 서버 및 도커를 사용할 기회를 주신 지도교수님께 무한한 감사를 드립니다 (DSBA lab)   Docker에서 image와 container를 생성하고 사용해봅시다.   Image  Docker Hub에 들어가시면 github에 repository를 관리하는 것 처럼, official하게 또 개인이 개인의 목적을 가지고 생성한 docker image들이 있습니다. Ubuntu, CentOS와 같은 OS는 물론이며 postgresql, mysql, mongoDB와 같은 툴들도 이미지화 되어 업로드 되어 있죠. 본 포스트에서는 deeplearning을 하기 위한 환경을 구축하려 합니다.   Deep learining framework을 사용하기 위한 가장 대표적인 이미지는 Deepo 입니다. 본 이미지에는 pytorch, tensorflow는 물론 keras, theano, 심지어 darknet까지 일반적으로 필요로 하는 라이브러리들을 모두 포함하고 있습니다. Docker hub에 있는 deepo를 사용하기 위해서는 git pull과 같이 docker pull ufoym/deepo:all-jupyter-py36-cu102와 같은 명령어를 사용하면 됩니다. 하지만 이와같이 모든 라이브러리가 들어가있는 이미지는 매우 무겁기 때문에(10G 이상) 각자 사용하는 환경에 맞게 custom을 해주시는것을 추천 드립니다.   Create Dockerfile  Dockerfile을 사용하여 custom image를 build하는 방법을 살펴보겠습니다. 우선 Dockerfile을 생성할 path로 이동해줍니다.   mkdir docker # 원하는 이름으로 변경 가능 vim Dockerfile # Dockerfile 생성 및 수정   위의 라인에서 vim 대신 vi나 nano처럼 편하신 명령어를 사용하셔서 Dockerfile을 아래처럼 생성하겠습니다.  # Dockerfile  # Pull bse image # - 저는 cuda 10.2에 ubuntu 18.04를 사용하고 있기 때문에 아래의 이미지를 가지고 왔습니다. # - docker hub에서 nvidia official image를 확인하시고 상황에 맞는 이미지를 가지고 오시면 되겠습니다. FROM nvidia/cuda:10.2-base-ubuntu18.04 ENV LC_ALL=C.UTF-8  # Install basic utilities # - ubuntu에 필요한 기본적인 것들을 설치해줍니다. # - 더 원하시는 util이 있으시면 아래 라인에 추가 혹은 삭제 해주시면 됩니다. RUN . /etc/os-release; \\                 printf \"deb http://ppa.launchpad.net/jonathonf/vim/ubuntu %s main\" \"$UBUNTU_CODENAME\" main | tee /etc/apt/sources.list.d/vim-ppa.list &amp;&amp; \\                 apt-key  adv --keyserver hkps://keyserver.ubuntu.com --recv-key 4AB0F789CBA31744CC7DA76A8CF63AD3F06FC659 &amp;&amp; \\                 apt-get update --fix-missing &amp;&amp; \\                 env DEBIAN_FRONTEND=noninteractive apt-get dist-upgrade --autoremove --purge --no-install-recommends -y \\                         build-essential \\                         bzip2 \\                         ca-certificates \\                         curl \\                         git \\                         libcanberra-gtk-module \\                         libgtk2.0-0 \\                         libx11-6 \\                         sudo \\                         graphviz \\                         vim-nox  # Install miniconda # - python에서 ML/DL을 사용할 것이기 때문에 여러모로 편리한 miniconda를 설치합니다. # - 환경변수도 함께 지정해줍니다. ENV LANG=C.UTF-8 LC_ALL=C.UTF-8 ENV PATH /opt/conda/bin:$PATH RUN apt-get install -y wget bzip2 ca-certificates \\     libglib2.0-0 libxext6 libsm6 libxrender1 \\     git mercurial subversion RUN wget --quiet https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda.sh &amp;&amp; \\     /bin/bash ~/miniconda.sh -b -p /opt/conda &amp;&amp; \\     rm ~/miniconda.sh &amp;&amp; \\     ln -s /opt/conda/etc/profile.d/conda.sh /etc/profile.d/conda.sh &amp;&amp; \\     echo \". /opt/conda/etc/profile.d/conda.sh\" &gt;&gt; ~/.bashrc &amp;&amp; \\     echo \"conda activate base\" &gt;&gt; ~/.bashrc RUN apt-get install -y curl grep sed dpkg &amp;&amp; \\     TINI_VERSION=`curl https://github.com/krallin/tini/releases/latest | grep -o \"/v.*\\\"\" | sed 's:^..\\(.*\\).$:\\1:'` &amp;&amp; \\     curl -L \"https://github.com/krallin/tini/releases/download/v${TINI_VERSION}/tini_${TINI_VERSION}.deb\" &gt; tini.deb &amp;&amp; \\     dpkg -i tini.deb &amp;&amp; \\     rm tini.deb &amp;&amp; \\     apt-get clean  # Install python packages # - python에서 사용할 패키지를 설치합니다. RUN pip install torch torchvision &amp;&amp; \\     pip install cython &amp;&amp; \\     pip install simplejson &amp;&amp; \\     conda install av -c conda-forge &amp;&amp; \\     pip install jupyterlab jupyterhub  # 혹시 python 패키지를 설치하실때 원래 사용하시던 패키지에서 생성한 requirements.txt 파일을 가지고 있고, # 해당 파일을 사용하여 패키지 설치를 하고싶으시다면 아래와 같은 라인들을 추가해주시면 됩니다.  COPY requirements.txt /tmp WORKDIR /tmp RUN pip install -r requirements.txt   Build docker image  생성한 Dockerfile을 사용하여 image를 빌드해보겠습니다.   # nvidia-docker를 사용하는 경우 nvidia-docker build -t $image_name .  # nvidia-docker 없이 docker만 사용하는 경우 docker build -t $image_name .   일반적으로 docker image 이름은 identity/name 정도로 지정해주시면 됩니다. 예를 들어 제가 가장 많이 사용하는 이미지 이름은 sw/base 이며, 생성한 이미지는 docker image ls를 통해 확인할 수 있습니다.   Create container  빌드한 Image 위에 container를 생성해보겠습니다.   # nvidia-docker를 사용하는 경우 nvidia-docker run -td --ipc=host --name $container_name -v ~/code/and/data/path/on/your/host:/folder/name/on/docker -p 8888:8888 -p 6006:6006 $Image_name  # nvidia-docker 없이 docker만 사용하는 경우 docker run --gpus all -td --ipc=host --name $container_name -v ~/code/and/data/path/on/your/host:/folder/name/on/docker -p 8888:8888 -p 6006:6006 $Image_name   option들은 아래와 같습니다:     –gpus all (docker): nvidia-docker는 자동으로 GPUs를 사용할 수 있지만, docker만 사용하는 경우 본 옵션을 사용해야만 GPU 자원들을 인식합니다.   -t: tty 입니다.   -d: detacth 입니다. 저는 컨테이너를 생성하고 후에 따로 실행하는 방식을 선호하기 때문에 해당 옵션을 항상 넣어둡니다.   –name: 생성 될 container의 이름 입니다.   -v: mount할 volumn의 이름입니다. Host OS에 code와 data가 담긴 폴더:container에 mount할 volumn이름 으로 지정하시면 됩니다.   -p: 사용할 port number 입니다.   이후 container를 올릴 image의 이름을 지정해주시면 해당 image위에 container가 생성됩니다.   Run docker container   위에서 생성한 container를 실행해보겠습니다.   # bash docker exec -it $container_name bash  # jupyterlab docker exec -it $container_name jupyter lab --no-browser --ip=0.0.0.0 --allow-root --NotebookApp.token= --notebook-dir='/folder/name/on/docker'   위의 라인을 실행시키면 생성한 container에 접속할 수 있습니다. 접속을 하신 후 ls를 통해 확인해 보시면 일반적인 우분투와 같이 폴더들이 잡혀 있는 것을 확인할 수 있으며, container를 생성할 때 -v 옵션으로 지정하셨던 volumn이 폴더 형태로 들어 있음을 보실 수 있습니다. 해당 폴더에 접근하시어 원하시는 code를 실행시켜 주시면 됩니다!  ","categories": ["Docker"],
        "tags": ["ubuntu","Docker","container","image"],
        "url": "http://localhost:4000/archive/use-docker",
        "teaser":null},{
        "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ViT)",
        "excerpt":"ICLR 2021에 “AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE” 라는 제목을 가진 논문이 게재되었습니다. Google research와 brain에서 힘을 합쳐 진행한 연구인데요, 역시 구글입니다. 폭발적인 후속 연구들을 이끈 논문, ViT에 대해 알아보겠습니다.   Transformer  Transformer 구조는 NLP domain에서 이미 지배적입니다. 일반적으로는 wikipedia와 같은 대용량의 corpus를 통하여 pre-train을 시킨 후 target domain의 data를 통하여 further pre-train을 진행합니다. 사실 further pre-train 단계는 optional한 감이 있지만 가능한 상황이라면 진행하는 것이 좋다로 정리가 되는 것 같습니다. 이후 최종적으로 task-specific dataset을 통하여 fine-tuning을 진행합니다. 현재 NLP는 BERT 전성시대 라고 불러도 과언이 아닙니다. 이러한 BERT와 지속적으로 후속 모델을 내고 있는 GPT 시리즈는 Transformer를 기반하고 있습니다. 결국, Transformer를 통해 seq2seq-based model의 한계들을 극복하여 모든 downstream task에서 SotA를 기록하고 있다고 볼 수 있습니다. 오늘 소개할 ViT는 Transformer를 vision에서 사용할 수 없을까? 라는 고민에서 시작되었습니다.   ViT  Model structure        Figure 1. Vision Transformer Model Overview   ViT의 구조는 Figure 1과 같습니다. 매우 단순합니다. Transformer를 알고 계신 분이라면 추가적으로 이해를 해야하는 부분이 거의 없습니다. Transformer에 익숙하지 않으신 분들은 본 블로그를 참고해 주세요.   Image to patch  ViT는 Transformer에 image를 feeding합니다. 일반적인 상황에서는 Transformer에 문장이 들어가죠. 전처리를 진행한 후 [“나”, “너”, “사랑”] 이라는 word token들이 input으로 들어가면, Transformer는 [“I”, “Love”, “You”]라는 token들로 구성된 output을 내뱉습니다. 결국 이미지 한 장을 한 문장으로 고려할때 우리는 저 문장 내부의 단어, 즉 token을 어떤식으로 구성해야 할 지를 해결해야 합니다. 본 논문에서 저자들은 이미지를 자르는 형태로 이를 해결합니다. 사실 이미지를 patch로 잘라 전처리를 진행하거나 model에 input으로 사용하는 연구들은 이전에도 많았습니다(역시 아는 것이 힘입니다). Figure 1의 &lt;Linear Projection of Flattened Patches&gt; 아래에 보면 작은 사이즈의 이미지들이 나열되어 있는 것을 볼 수 있습니다. 여기가 이미지를 patch로 잘라 한 patch를 하나의 token으로 사용함을 보여주는 부분입니다. Patch의 사이즈는 원본 이미지가 $x \\in \\mathbb{R}^{H\\times W\\times C}$ 인 경우에 $x_p \\in \\mathbb{R}^{N\\times (P^2\\times C)}$ 로 정해집니다. 예를 들어 9x9 크기의 color image가 있을 때, patch size를 3으로 지정하면 3x3 ($P^2$) patch가 9 ($N$)개 생성되겠죠. 그럼 이 9 개의 patch를 token으로 고려해서 vector로 만든 후에 &lt;Linear Projection of Flattened Patches&gt; module에 넣어주게 됩니다. 비록 image는 word token과 다르게 연속적이지만, 모델이 보다 의미 있는 input을 받게 하기 위해서 embedding을 거칩니다.   다만 한 가지 “실험적으로” 주의 하실 점은, patch를 vector로 만드는 부분입니다. 일반적으로 flatten을 한다고 생각하기 쉽고, 몇몇 공개된 ViT code에서도 flatten을 사용하고 있으나 모델의 성능을 위해서 kernel size와 stride가 patch size이며 output dim이 vector size로 지정된 Conv2d를 사용해야 합니다. 위의 예시와 같은 상황에서는 아래와 같이 지정된 kernel을 통하여 각 patch를 256차원의 vector로 변경할 수 있습니다:     torch.nn.Conv2d(3, out_channels=256, kernel_size=3, stride=3)    Position Embedding  다음으로 구성해야하는 input은 Position embedding 입니다. 이는 input patch의 위치를 모델이 알 수 있게끔 해주는 역할을 합니다.        Figure 2. Position Embedding Example  위의 Figure 2에서 볼 수 있듯이 이미지를 대상으로 하는 position embedding은 1차원, 혹은 2차원으로 구성할 수 있습니다. 본 논문에서는 두 가지 모두 사용해본 결과 성능에는 큰 차이가 없다고 말합니다. ViT position embedding의 특징은 Figure 2와 같이 patch의 실질적인 위치를 사용하거나 Transformer의 position embedding처럼 계산에 의한 값을 사용하지 않는다는 것 입니다. ViT의 position embedding은 random하게 초기화 되어 모델의 학습이 진행되는 동안 지속적으로 update 됩니다. PyTorch에서는 torch.nn.Parameter() 로 지정됩니다.   실험 결과 그림 추가   cls token 언급     Experiments  ","categories": ["ViT"],
        "tags": ["ViT","vision transformer","transformer","ICLR","inductive bias"],
        "url": "http://localhost:4000/archive/ViT",
        "teaser":null}]
