<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.13.0 by Michael Rose
  Copyright 2013-2018 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Introduction to Deep Learning-MIT 6.S191 - 지도교수님의 권유로 시작하는 블로그</title>
<meta name="description" content="AGENDA1. Introduction of basic concepts for Deep learning">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="지도교수님의 권유로 시작하는 블로그">
<meta property="og:title" content="Introduction to Deep Learning-MIT 6.S191">
<meta property="og:url" content="http://localhost:4000/archive/Introduction-to-Deep-Learning-(MIT-6.S191)">


  <meta property="og:description" content="AGENDA1. Introduction of basic concepts for Deep learning">







  <meta property="article:published_time" content="2019-06-01T00:00:00+09:00">





  

  


<link rel="canonical" href="http://localhost:4000/archive/Introduction-to-Deep-Learning-(MIT-6.S191)">







  <script type="application/ld+json">
    {
      "@context": "http://schema.org",
      "@type": "Person",
      "name": "lepoeme20",
      "url": "http://localhost:4000/",
      "sameAs": null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="지도교수님의 권유로 시작하는 블로그 Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

<script data-ad-client="ca-pub-6150728808077226" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    
      <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$'] ],
    processEscapes: true,
  }
});
MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
	  alert("Math Processing Error: "+message[1]);
	});
MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
	  alert("Math Processing Error: "+message[1]);
	});
</script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

    
  </head>

  <body class="layout--single">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <a class="site-title" href="/">지도교수님의 권유로 시작하는 블로그</a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/year-archive/" >Posts</a>
            </li><li class="masthead__menu-item">
              <a href="/categories-archive/" >Category</a>
            </li><li class="masthead__menu-item">
              <a href="/profile/" >Profile</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16">
            <path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path>
          </svg>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  
  
    
      
      <h3>MIT 6.S191</h3>
      
    
      
      <h3>Lecture 1</h3>
      
    
      
      <h3>Year : 2019</h3>
      
    
    
  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Introduction to Deep Learning-MIT 6.S191">
    <meta itemprop="description" content="AGENDA1. Introduction of basic concepts for Deep learning">
    <meta itemprop="datePublished" content="June 01, 2019">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Introduction to Deep Learning-MIT 6.S191
</h1>
          
            <!-- <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  4 minute read
</p> -->
            <p class="page__meta"><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated: <time datetime="2019-06-01T00:00:00+09:00">June 01, 2019</time></p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> Page Index</h4></header>
              <ul class="toc__menu">
  <li><a href="#deep-learning은-무엇인가">Deep learning은 무엇인가?</a></li>
  <li><a href="#feed-forward-propagation">Feed forward propagation</a></li>
  <li><a href="#building-neural-network-with-perceptrons">Building neural network with Perceptrons</a></li>
  <li><a href="#applying-neural-networks">Applying Neural Networks</a></li>
  <li><a href="#training-neural-networks">Training Neural Networks</a></li>
  <li><a href="#neural-networks-in-practice">Neural Networks in practice</a>
    <ul>
      <li><a href="#optimization">Optimization</a></li>
      <li><a href="#overfitting">Overfitting</a></li>
      <li><a href="#regulaization">Regulaization</a></li>
    </ul>
  </li>
  <li><a href="#마무리">마무리</a></li>
</ul>
            </nav>
          </aside>
        
        <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">AGENDA</span>
<span class="s">1. Introduction of basic concepts for Deep learning</span>
</code></pre></div></div>

<p>이미 너무 유명하고 우리에게 친숙한 Stanford 에서 제공하는 CS231n 수업 외에도 deep learning을 학습하기 위한 좋은 material이 있지 않을까? 라는 생각에서 시작하여 2019년 MIT에서 제공하는 deep learning 수업을 review 해볼까 합니다. 전반적으로 CS231n보다 범위가 넓고 깊이가 얕은 특성이 있는 듯 합니다.</p>

<p>사족이지만 해당 class의 final project의 결과에 따라 우수한 학생에게는 RTX 2080 Ti를 상으로 제공한다고 하네요…. <del>MIT 만세</del></p>

<h2 id="deep-learning은-무엇인가"><b>Deep learning은 무엇인가?</b></h2>
<blockquote>
  <p>“An intelligence as the ability to process information to inform future decision.”</p>
</blockquote>

<p>수업 진행자가 하는 말을 그대로 가지고 와보았습니다. 이는 결국 데이터의 패턴을 잘 파악하여 미래 데이터에 적용한 후 정보를 추출하는 일련의 과정이 됩니다. 그렇다면 어떠한 점들이 machine learning과의 차이점일까요?</p>

<figure class="align-center">
  <img src="http://localhost:4000//assets/images/MIT6.S191/lec1/ai-ml-dl.png" alt="" />
  <figcaption>Fig 1. The differences between AI, ML, and DL</figcaption>
</figure>

<p>위의 그림은 Artificial intelligence (AI), machine learning (ML), 그리고 deep learning (DL)의 차이를 나타내고 있습니다. AI가 가장 큰 범위의 개념으로 기계가 사람과 같이 행동할 수 있는 모든 기술을 의미하고 있으며 ML은 AI에 속하는 개념으로 어느정도 자동화가 되어 학습된 패턴을 통하여 미래 데이터에 대하여 의사결정을 내리는 행위를 의미합니다. 마지막으로 최근 비약적인 속도로 발전하고 있는 DL분야는 ML에 속하는 굉장히 좁은 영역으로 ML에 비해 사람의 손을 덜 타는, 예를 들어 image domain에서 이미지를 잘 표현하기 위하여 여러 방법으로 hand-crafted features를 추출하는 것이 기존 ML의 중요한 이슈였다면 deep learning에서는 모델이 알아서 feature를 뽑도록 합니다, 보다 지능적인 프로세스 라고 표현할 수 있을 것 같습니다.</p>

<p>그렇다면 우리는 왜 deep learning을 해야 할까요?</p>

<p>가장 단순하게 이야기 하면 기존에 사용하던 hand-crafed features에 비하여 우리의 손을 거치지 않고 기계가 직접 중요하다고 판단한 features를 사용하는 것이 많은 분야에서 성능이 ‘훨씬’ 더 좋기 때문일 것입니다.</p>

<h2 id="feed-forward-propagation"><b>Feed forward propagation</b></h2>
<figure class="align-center">
  <img src="http://localhost:4000//assets/images/MIT6.S191/lec1/MLP.png" alt="" />
  <figcaption>Fig 2. MLP</figcaption>
</figure>

<p>일반적인 neural network의 feed forward propagation은 위 그림에서 보이는 <script type="math/tex">x_1, x_2, x_3</script>와 같은 input들과 <script type="math/tex">w_11, w_12, w_32</script>와 같은 weight들의 dot product 로 나온 값을 비선형 함수에 input으로 사용하는 과정들을 반복한 후 최종 output이 산출 됩니다. 이 과정에서 추가되는 bias term은 activation function을 shift하는 효과를 준다고 하네요.</p>

<p>Neural network는 사실 Fig 2.에서 가운데 위치하는 hidden layer를 여려층 쌓음으로 매우 복잡한 결합함수 형태를 가집니다. 만약 선형함수를 계속 결합한다면 어떻게 될까요? 최종적인 함수의 형태도 선형 함수가 되겠죠? 그렇다면 힘들게 쌓아올린 hidden layer들이 큰 의미를 갖기 어려울 것입니다. 이러한 문제를 해결하기 위하여 neural network에서는 비선형함수인 activation function을 사용하여 모델에 비선형성을 추가해 줍니다. 이러한 특성 덕분에 neural net은 매우 비선형적인 현실 데이터에서도 높은 성능을 지닐 수 있게 됩니다.</p>

<p>가장 유명하고 오래 쓰인 activation function은 sigmoid 함수 입니다. sigmoid 함수는 인풋을 0-1로 바꾸어주기 때문에 모델을 통하여 확률을 구하고자 하는 경우에 상당히 유용한 함수가 될것이라고 강의자는 말하고 있습니다. 또 하나의 유용하고 유명한 함수는 Rectified Liner Unit (ReLU) 입니다. ReLU의 경우 0보다 작은 input은 무시하고 0보다 큰 값에 대하여서는 인풋이 아웃풋이 되는 <script type="math/tex">y=x</script> 형태를 띄고 있습니다. 이러한 특성 때문에 picewise linearity라고 불리죠. 이렇게 단순한 함수로 충분한지 의심이 드실텐데요, ReLU는 굉장히 선형적으로 생겼음에도 불구하고 모든 중요한 속성을 보존함과 동시에 연산이 쉽기 때문에 최근 대부분의 모델에서 사용하고 있습니다.</p>

<h2 id="building-neural-network-with-perceptrons"><strong>Building neural network with Perceptrons</strong></h2>
<p>그림 2에서 <script type="math/tex">h_1</script>은 inputs과 weights를 사용한 weighted sum 형태가 됩니다. 이렇게 weighted sum이 된 값에 activation function을 거쳐 neuron 하나의 output이 산출되죠. Hidden unit이 많아져도 neuron마다 찬찬히 살펴보면 단순히 하나의 neuron을 가진 perceptron network (그림 2) 와 다를바가 없습니다. 물론 weights들의 값이 다르기 때문에 <script type="math/tex">h1, h2</script>의 값은 각각 다르겠지요. 이렇게 빽빽하게 weight가 연결된 구조를 우리는 dense layer라고 부릅니다. Deep Neural Network또한 이러한 구조와 크게 다르지 않은데, 단순히 계속적으로 엄청 많이 계속 계속 이러한 dense layer을 stack하게 되면 그것이 DNN이 됩니다.</p>

<h2 id="applying-neural-networks"><strong>Applying Neural Networks</strong></h2>
<p>굉장히 뜨겁고 모든 것을 해줄것만 같은 deep learning model또한 물론 만능이 아닙니다. 사람이 봤을때도 합리적인 판단을 모델이 하기 위해서 우리는 모델을 ‘잘’ 학습 시켜야만 하죠. 우리는 실제 정답을 ground truth, model이 예측한 정답을 predicted output이라 부르며 이 둘의 차이를 점차 줄여나가는 과정을 학습이라 부릅니다. 이처럼 실제 정답과 모델이 추측한 정답을 줄여나가기 위하여 세우는 함수인 Loss function (NLL, cross-entrophy, MSE와 같은 다양한 방법들이 있습니다)을 정의하는 것은 machine learning의 art에 속합니다.</p>

<h2 id="training-neural-networks"><strong>Training Neural Networks</strong></h2>
<p>위에서 우리는 모델을 잘 학습 시켜야 한다고 말하였습니다. 그렇다면 학습을 시킨다는 것은 무엇을 의미하는 것일까요? 모델을 학습 시키는 궁극적인 목표는 모델이 가지고 있는 파라미터 W (weights)를 최적화 하는 것입니다.
여기서 최적화의 의미는 predicted output과 ground truth의 차이를 최소화 하는 것이 됩니다.
하지만 Neural network는 위에서 말한 것 처럼 상당히 많은 layer (function)가 stack되어 있기 때문에 최적해를 closed form으로 구할 수 없는 문제점을 가지고 있습니다.
이러한 어려운 상황에서 해를 구하기 위해서 우리는 gradient descent와 backpropagation을 사용합니다.</p>

<h2 id="neural-networks-in-practice"><strong>Neural Networks in practice</strong></h2>
<h3 id="optimization">Optimization</h3>
<p>DNN은 매우 복잡한 결합함수 형태를 띄고 있기 때문에 구조적으로 매우 많은 local minimum이 존재하고 initial point에 따라 이런 local minimum에 빠질수도, 저런 local minimum에 빠질 수도 있습니다.
이 때문에 optimization (adam, adadelta, RMSprop, …) 연구도 매우 활발히 진행 되었으나 최근에는 주춤하는 모습인것 같습니다. 본인이 선호하는 optimizer를 사용하는 추세이며 ‘local optimum에 빠지더라도 충분히 좋다’ 라는게 <del>학계의 정설</del> 입니다.</p>

<h3 id="overfitting">Overfitting</h3>
<p>Model은 새로운 데이터 (test set)에 대해서도 강건함을 목표로 학습됩니다. 다시 말해, 처음 보는 데이터에 대해서도 학습때의 성능을 보여주길 바라며 모델을 학습 시키죠. 하지만 간혹 학습 데이터를 완전히 외워버리는 모델들이 등장하곤 하는데 이러한 경우 새로운 데이터에 대한 성능은 현저히 떨어질 수 밖에 없습니다. 학습 데이터를 온전히 외워버리기 때문에 새로운 데이터에 대해서 대처하지 못하는 상황이기 때문이죠. 이러한 현상을 우리는 overfitting이라 부릅니다.</p>

<h3 id="regulaization">Regulaization</h3>
<p>이러한 overfitting 현상을 완화하기 위하여 사용하는 방법이 regularization입니다. 
가장 유명하고 널리 사용되는 방법은 dropout으로 특정 neuron을 deactivation 시키는 방식입니다. 이러한 방식을 통하여 모델이 특정 witght에 온전히 의존하지 못하게 하는 것이죠.</p>

<p>두 번째 방식은 early stopping이라 불리는 트릭 입니다.
이름과 같이 말 그대로 빠르게 학습을 중단시키는 방식인데 그 기준은 training error는 계속 낮아지지만 validation error가 올라가는 지점입니다. Training set은 학습에 계속 사용 되지만 validation set은 학습에 사용되지 않기 때문에 validation error가 올라간다는 것은 모델이 overfitting되어 test set의 performance도 떨어질 것이라는 가정을 담고 있는 것이죠.</p>

<h2 id="마무리"><strong>마무리</strong></h2>
<p>기본적인 컨셉을 잡는 강의었습니다. 구글이나 유튜브에 ‘MIT 6.S191’를 검색하시면 슬라이드와 강의 영상을 쉽게 찾으실 수 있으니 관심 있으신 분들은 꼭 본 강의를 참고 부탁드립니다!</p>

        
      </section>

      <footer class="page__meta">
        
        
  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/tags/#activation-function" class="page__taxonomy-item" rel="tag">activation function</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#basic" class="page__taxonomy-item" rel="tag">Basic</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#deep-learning" class="page__taxonomy-item" rel="tag">Deep learning</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#lecture" class="page__taxonomy-item" rel="tag">Lecture</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#mit" class="page__taxonomy-item" rel="tag">MIT</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#optimization" class="page__taxonomy-item" rel="tag">optimization</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#overfitting" class="page__taxonomy-item" rel="tag">overfitting</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#regulaization" class="page__taxonomy-item" rel="tag">Regulaization</a>
    
    </span>
  </p>




  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/categories/#basic" class="page__taxonomy-item" rel="tag">Basic</a><span class="sep">, </span>
    
      
      
      <a href="/categories/#lectures" class="page__taxonomy-item" rel="tag">Lectures</a>
    
    </span>
  </p>


        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2019-06-01T00:00:00+09:00">June 01, 2019</time></p>
        
      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=Introduction+to+Deep+Learning-MIT+6.S191%20http%3A%2F%2Flocalhost%3A4000%2Farchive%2FIntroduction-to-Deep-Learning-%28MIT-6.S191%29" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Farchive%2FIntroduction-to-Deep-Learning-%28MIT-6.S191%29" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Farchive%2FIntroduction-to-Deep-Learning-%28MIT-6.S191%29" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/archive/FGSM" class="pagination--pager" title="Explaining and Harnessing Adversarial Examples
">Previous</a>
    
    
      <a href="/archive/Recurrent-Neural-Networks-(MIT-6.S191)" class="pagination--pager" title="Recurrent Neural Networks-MIT 6.S191
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You may also enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/archive/control-git-on-terminal" rel="permalink">Control git on terminal
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-calendar-alt" aria-hidden="true"></i> Updated: <time datetime="2020-02-07T00:00:00+09:00">February 07, 2020</time></p>
    
    <p class="archive__item-excerpt" itemprop="description">이번 포스터에서는 github repository와 local folder를 연동하여 repository를 관리하는 방법에 대하여 알아보겠습니다.

</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/archive/rotate-display-on-terminal" rel="permalink">Rotate display on terminal
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-calendar-alt" aria-hidden="true"></i> Updated: <time datetime="2020-02-06T00:00:00+09:00">February 06, 2020</time></p>
    
    <p class="archive__item-excerpt" itemprop="description">이번 포스터에서는 terminal에서 display를 회전하는 방법에 대하여 살표보겠습니다.

</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/archive/Set-up-ubuntu-for-deeplearning" rel="permalink">Setup ubuntu for deeplearning
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-calendar-alt" aria-hidden="true"></i> Updated: <time datetime="2020-02-06T00:00:00+09:00">February 06, 2020</time></p>
    
    <p class="archive__item-excerpt" itemprop="description">본 포스터는 https://www.pytorials.com 를 참조하였습니다.


</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/archive/Deep-Learning-for-Computer-Vision-(MIT-6.S191)" rel="permalink">Deep Learning for Computer Vision-MIT 6.S191
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-calendar-alt" aria-hidden="true"></i> Updated: <time datetime="2019-06-13T00:00:00+09:00">June 13, 2019</time></p>
    
    <p class="archive__item-excerpt" itemprop="description">AGENDA
1. Introduction of basic concepts for CNN


</p>
  </article>
</div>
        
      </div>
    </div>
  
  
</div>
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><input type="text" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
    <div id="results" class="results"></div></div>
      </div>
    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2020 lepoeme20. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.3.1/js/all.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




    
  <script>
    var disqus_config = function () {
      this.page.url = "http://localhost:4000/archive/FGSM";  // Replace PAGE_URL with your page's canonical URL variable
      this.page.identifier = "/archive/FGSM"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    (function() { // DON'T EDIT BELOW THIS LINE
      var d = document, s = d.createElement('script');
      s.src = 'https://lepoeme20.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


  


<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-140652840-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-140652840-1');
</script>


  </body>
</html>
